{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae03b3c-c339-4c14-a1f8-bce72af93aa5",
   "metadata": {},
   "source": [
    "## Chatbot with RAG with Evaluation on Gradio UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c65a0-9acc-476c-88d9-3072fe541d7d",
   "metadata": {},
   "source": [
    "This notebook aims to create a customized chatbot, leveraging RAG (Retrieval Augmented Generation) technique that incorporate preliminary domain context from ArXiv papers, which significantly enhances the response relevance and accuracy. \n",
    "\n",
    "RAG combines the strengths of both retrieval-based and generation-based models to improve the quality and efficiency of text generation.\n",
    "\n",
    "The basic idea behind RAG is to use a retrieval model to find relevant information from a large corpus of text which stored in the vector database (e.g. FAISS), and then use a generation model to generate new text that incorporates the retrieved information. This can help to improve the coherence and accuracy of the generated text, as well as reduce the risk of generating irrelevant or repetitive content.\n",
    "\n",
    "The Chatbot is able to utilize both basic chain and RAG-enhanced chain to generate response per user query. Evaluation is empowered to compare two chain performance based on sythethic questions from Judge LLM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15ecbf-18d6-4eff-90d0-e76b272f6a4c",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "The notebook applies NVIDIA AI foundation models, which requires NVIDIA API key. \n",
    "\n",
    "Langchain is a popular LLM orchestration tool to manage the LLM workflow components, which connects the vector database to LLM model. This notebook will be using the LangChain Expression Language (LCEL) from basic chain specification to more advanced dialog management practices.\n",
    "\n",
    "LangServe is used to deploy the Langchain workflow to application server, to create and distribute accessible API routes.\n",
    "\n",
    "Gradio is applied to create customized UI, with functions to allow streaming user query, dynamic selection of basic chain or RAG-enhanced chain, and evaluate the performance of two chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb9632e-104f-480a-b35f-771628d27353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "    \n",
    "## If you're in colab and encounter a typing-extensions issue,\n",
    "##  restart your runtime and try again\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d7d1e1-d502-4124-b7da-8dbc4fab98ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved NVIDIA_API_KEY beginning with \"nvapi-uZd...\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
       " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
       " 'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
       " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
       " 'playground_seamless': '72ad9555-2e3d-4e73-9050-a37129064743',\n",
       " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
       " 'playground_llama2_code_70b': '2ae529dc-f728-4a46-9b8d-2697213666d8',\n",
       " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
       " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
       " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
       " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
       " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
       " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92',\n",
       " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
       " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
       " 'playground_mamba_chat': '381be320-4721-4664-bd75-58f8783b43c7',\n",
       " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
       " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
       " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
       " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
       " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
       " 'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
       " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "\n",
    "from getpass import getpass\n",
    "import requests\n",
    "import os\n",
    "\n",
    "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
    "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
    "    try: \n",
    "        assert not hard_reset\n",
    "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
    "        assert response.get('nvapi_key')\n",
    "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
    "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
    "    except: pass\n",
    "    hard_reset = False\n",
    "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
    "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
    "\n",
    "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "NVEModel().available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6de3f5-094c-474a-b869-fd9515307ad2",
   "metadata": {},
   "source": [
    "### Summary of RAG Workflows\n",
    "\n",
    "> **Incorporate External Documents into LLM Workflow:**\n",
    "- Divide **each document** into chunks and process them into useful messages.\n",
    "- Generate semantic embedding for each new document chunk.\n",
    "- Add the chunk bodies to **a scalable vector database for fast retrieval**.\n",
    "- Query the **vector database** for relevant chunks to fill in the LLM context.\n",
    "\n",
    "\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1cFbKbVvLLnFPs3yWCKIuzXkhBWh6nLQY\" width=1200px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb6e27-41f8-4076-b97f-f4f5ecd9b25b",
   "metadata": {},
   "source": [
    "### Loading And Chunking External Documents\n",
    "\n",
    "The following code block incorporate recent Arxiv papers to load in for the RAG chain. A few simplifying assumptions and additional processing steps are included to help improve naive RAG performance:\n",
    "\n",
    "- Documents are cut off prior to the \"References\" section if one exists. This will keep the system from considering the citations and appendix sections, which tend to be long and distracting.\n",
    "\n",
    "- A chunk that lists the available documents is inserted to provide a high-level view of all available documents in a single chunk. \n",
    "\n",
    "- Additionally, the metadata entries are also inserted to provide general information. Ideally, there would also be some synthetic chunks that merge the metadata into interesting cross-document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21df61a3-58b0-42b6-a324-11e2fd3a7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Chunking Documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Large Language Models: A Survey</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Generative AI in the Construction Industry: A State-of-the-art Analysis</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Financial Report Chunking for Effective Retrieval Augmented Generation </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Large Language Models: A Survey\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Generative AI in the Construction Industry: A State-of-the-art Analysis\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Financial Report Chunking for Effective Retrieval Augmented Generation \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - Metadata: {'Published': '2021-04-12', 'Title': 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks', 'Authors': 'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela', 'Summary': 'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'}\n",
      " - # Chunks: 42\n",
      "\n",
      "Document 1\n",
      " - Metadata: {'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}\n",
      " - # Chunks: 43\n",
      "\n",
      "Document 2\n",
      " - Metadata: {'Published': '2024-01-05', 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang', 'Summary': \"Large Language Models (LLMs) demonstrate significant capabilities but face\\nchallenges such as hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the models,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval , the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces the metrics and benchmarks for assessing RAG\\nmodels, along with the most up-to-date evaluation framework. In conclusion, the\\npaper delineates prospective avenues for research, including the identification\\nof challenges, the expansion of multi-modalities, and the progression of the\\nRAG infrastructure and its ecosystem.\"}\n",
      " - # Chunks: 110\n",
      "\n",
      "Document 3\n",
      " - Metadata: {'Published': '2024-02-12', 'Title': 'PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models', 'Authors': 'Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia', 'Summary': 'Large language models (LLMs) have achieved remarkable success due to their\\nexceptional generative capabilities. Despite their success, they also have\\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\\nmitigate those limitations. In particular, given a question, RAG retrieves\\nrelevant knowledge from a knowledge database to augment the input of the LLM.\\nFor instance, the retrieved knowledge could be a set of top-k texts that are\\nmost semantically similar to the given question when the knowledge database\\ncontains millions of texts collected from Wikipedia. As a result, the LLM could\\nutilize the retrieved knowledge as the context to generate an answer for the\\ngiven question. Existing studies mainly focus on improving the accuracy or\\nefficiency of RAG, leaving its security largely unexplored. We aim to bridge\\nthe gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge\\npoisoning attacks to RAG, where an attacker could inject a few poisoned texts\\ninto the knowledge database such that the LLM generates an attacker-chosen\\ntarget answer for an attacker-chosen target question. We formulate knowledge\\npoisoning attacks as an optimization problem, whose solution is a set of\\npoisoned texts. Depending on the background knowledge (e.g., black-box and\\nwhite-box settings) of an attacker on the RAG, we propose two solutions to\\nsolve the optimization problem, respectively. Our results on multiple benchmark\\ndatasets and LLMs show our attacks could achieve 90% attack success rates when\\ninjecting 5 poisoned texts for each target question into a database with\\nmillions of texts. We also evaluate recent defenses and our results show they\\nare insufficient to defend against our attacks, highlighting the need for new\\ndefenses.'}\n",
      " - # Chunks: 73\n",
      "\n",
      "Document 4\n",
      " - Metadata: {'Published': '2024-02-13', 'Title': 'Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning', 'Authors': 'Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu', 'Summary': 'Large Language Models~(LLMs) have gained immense popularity and are being\\nincreasingly applied in various domains. Consequently, ensuring the security of\\nthese models is of paramount importance. Jailbreak attacks, which manipulate\\nLLMs to generate malicious content, are recognized as a significant\\nvulnerability. While existing research has predominantly focused on direct\\njailbreak attacks on LLMs, there has been limited exploration of indirect\\nmethods. The integration of various plugins into LLMs, notably Retrieval\\nAugmented Generation~(RAG), which enables LLMs to incorporate external\\nknowledge bases into their response generation such as GPTs, introduces new\\navenues for indirect jailbreak attacks.\\n  To fill this gap, we investigate indirect jailbreak attacks on LLMs,\\nparticularly GPTs, introducing a novel attack vector named Retrieval Augmented\\nGeneration Poisoning. This method, Pandora, exploits the synergy between LLMs\\nand RAG through prompt manipulation to generate unexpected responses. Pandora\\nuses maliciously crafted content to influence the RAG process, effectively\\ninitiating jailbreak attacks. Our preliminary tests show that Pandora\\nsuccessfully conducts jailbreak attacks in four different scenarios, achieving\\nhigher success rates than direct attacks, with 64.3\\\\% for GPT-3.5 and 34.8\\\\%\\nfor GPT-4.'}\n",
      " - # Chunks: 36\n",
      "\n",
      "Document 5\n",
      " - Metadata: {'Published': '2024-02-15', 'Title': 'Generative AI in the Construction Industry: A State-of-the-art Analysis', 'Authors': 'Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed', 'Summary': 'The construction industry is a vital sector of the global economy, but it\\nfaces many productivity challenges in various processes, such as design,\\nplanning, procurement, inspection, and maintenance. Generative artificial\\nintelligence (AI), which can create novel and realistic data or content, such\\nas text, image, video, or code, based on some input or prior knowledge, offers\\ninnovative and disruptive solutions to address these challenges. However, there\\nis a gap in the literature on the current state, opportunities, and challenges\\nof generative AI in the construction industry. This study aims to fill this gap\\nby providing a state-of-the-art analysis of generative AI in construction, with\\nthree objectives: (1) to review and categorize the existing and emerging\\ngenerative AI opportunities and challenges in the construction industry; (2) to\\npropose a framework for construction firms to build customized generative AI\\nsolutions using their own data, comprising steps such as data collection,\\ndataset curation, training custom large language model (LLM), model evaluation,\\nand deployment; and (3) to demonstrate the framework via a case study of\\ndeveloping a generative model for querying contract documents. The results show\\nthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,\\n9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study\\nprovides academics and construction professionals with a comprehensive analysis\\nand practical framework to guide the adoption of generative AI techniques to\\nenhance productivity, quality, safety, and sustainability across the\\nconstruction industry.'}\n",
      " - # Chunks: 132\n",
      "\n",
      "Document 6\n",
      " - Metadata: {'Published': '2024-02-11', 'Title': 'Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models', 'Authors': 'Zhibo Hu, Chen Wang, Yanfeng Shu, Helen, Paik, Liming Zhu', 'Summary': \"The robustness of large language models (LLMs) becomes increasingly important\\nas their use rapidly grows in a wide range of domains. Retrieval-Augmented\\nGeneration (RAG) is considered as a means to improve the trustworthiness of\\ntext generation from LLMs. However, how the outputs from RAG-based LLMs are\\naffected by slightly different inputs is not well studied. In this work, we\\nfind that the insertion of even a short prefix to the prompt leads to the\\ngeneration of outputs far away from factually correct answers. We\\nsystematically evaluate the effect of such prefixes on RAG by introducing a\\nnovel optimization technique called Gradient Guided Prompt Perturbation (GGPP).\\nGGPP achieves a high success rate in steering outputs of RAG-based LLMs to\\ntargeted wrong answers. It can also cope with instructions in the prompts\\nrequesting to ignore irrelevant context. We also exploit LLMs' neuron\\nactivation difference between prompts with and without GGPP perturbations to\\ngive a method that improves the robustness of RAG-based LLMs through a highly\\neffective detector trained on neuron activation triggered by GGPP generated\\nprompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of\\nour methods.\"}\n",
      " - # Chunks: 60\n",
      "\n",
      "Document 7\n",
      " - Metadata: {'Published': '2024-02-10', 'Title': 'REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models', 'Authors': 'Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, Chengwei Pan', 'Summary': 'The integration of multimodal Electronic Health Records (EHR) data has\\nsignificantly improved clinical predictive capabilities. Leveraging clinical\\nnotes and multivariate time-series EHR, existing models often lack the medical\\ncontext relevent to clinical tasks, prompting the incorporation of external\\nknowledge, particularly from the knowledge graph (KG). Previous approaches with\\nKG knowledge have primarily focused on structured knowledge extraction,\\nneglecting unstructured data modalities and semantic high dimensional medical\\nknowledge. In response, we propose REALM, a Retrieval-Augmented Generation\\n(RAG) driven framework to enhance multimodal EHR representations that address\\nthese limitations. Firstly, we apply Large Language Model (LLM) to encode long\\ncontext clinical notes and GRU model to encode time-series EHR data. Secondly,\\nwe prompt LLM to extract task-relevant medical entities and match entities in\\nprofessionally labeled external knowledge graph (PrimeKG) with corresponding\\nmedical knowledge. By matching and aligning with clinical standards, our\\nframework eliminates hallucinations and ensures consistency. Lastly, we propose\\nan adaptive multimodal fusion network to integrate extracted knowledge with\\nmultimodal EHR data. Our extensive experiments on MIMIC-III mortality and\\nreadmission tasks showcase the superior performance of our REALM framework over\\nbaselines, emphasizing the effectiveness of each module. REALM framework\\ncontributes to refining the use of multimodal EHR data in healthcare and\\nbridging the gap with nuanced medical context essential for informed clinical\\npredictions.'}\n",
      " - # Chunks: 46\n",
      "\n",
      "Document 8\n",
      " - Metadata: {'Published': '2024-02-01', 'Title': 'HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA', 'Authors': 'Xinyue Chen, Pengyu Gao, Jiangjiang Song, Xiaoyang Tan', 'Summary': 'As language model agents leveraging external tools rapidly evolve,\\nsignificant progress has been made in question-answering(QA) methodologies\\nutilizing supplementary documents and the Retrieval-Augmented Generation (RAG)\\napproach. This advancement has improved the response quality of language models\\nand alleviates the appearance of hallucination. However, these methods exhibit\\nlimited retrieval accuracy when faced with massive indistinguishable documents,\\npresenting notable challenges in their practical application. In response to\\nthese emerging challenges, we present HiQA, an advanced framework for\\nmulti-document question-answering (MDQA) that integrates cascading metadata\\ninto content as well as a multi-route retrieval mechanism. We also release a\\nbenchmark called MasQA to evaluate and research in MDQA. Finally, HiQA\\ndemonstrates the state-of-the-art performance in multi-document environments.'}\n",
      " - # Chunks: 37\n",
      "\n",
      "Document 9\n",
      " - Metadata: {'Published': '2024-02-10', 'Title': 'Financial Report Chunking for Effective Retrieval Augmented Generation', 'Authors': 'Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, Renyu Li', 'Summary': 'Chunking information is a key step in Retrieval Augmented Generation (RAG).\\nCurrent research primarily centers on paragraph-level chunking. This approach\\ntreats all texts as equal and neglects the information contained in the\\nstructure of documents. We propose an expanded approach to chunk documents by\\nmoving beyond mere paragraph-level chunking to chunk primary by structural\\nelement components of documents. Dissecting documents into these constituent\\nelements creates a new way to chunk documents that yields the best chunk size\\nwithout tuning. We introduce a novel framework that evaluates how chunking\\nbased on element types annotated by document understanding models contributes\\nto the overall context and accuracy of the information retrieved. We also\\ndemonstrate how this approach impacts RAG assisted Question & Answer task\\nperformance. Our research includes a comprehensive analysis of various element\\ntypes, their role in effective information retrieval, and the impact they have\\non the quality of RAG outputs. Findings support that element type based\\nchunking largely improve RAG results on financial reporting. Through this\\nresearch, we are also able to answer how to uncover highly accurate RAG.'}\n",
      " - # Chunks: 35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Incorporate Arxiv papers as external documents\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    # ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    # ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    # ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    # ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    # ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    # ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
    "    ArxivLoader(query=\"2312.10997\").load(),  ## RAG for LLM\n",
    "    ArxivLoader(query=\"2402.07867\").load(),  ## Knowledge Poisoning Attacks to RAG\n",
    "    ArxivLoader(query=\"2402.08416\").load(),  ## Jailbreak attack RAG\n",
    "    ArxivLoader(query=\"2402.09939\").load(), ## RAG in Construction\n",
    "    ArxivLoader(query=\"2402.07179\").load(), ## Prompt Perturbation in RAG\n",
    "    ArxivLoader(query=\"2402.07016\").load(), ## RAG in Health Records\n",
    "    ArxivLoader(query=\"2402.01767\").load(), ## Hierarchical Contextual RAG\n",
    "    ArxivLoader(query=\"2402.05131\").load(), ## Financial Report RAG\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = doc[0].page_content\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - Metadata: {chunks[0].metadata}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a86ce-0340-46d9-8de4-b863172d4136",
   "metadata": {},
   "source": [
    "### Construct Document Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08c22d-b189-4d2c-bfb5-7737a20cc381",
   "metadata": {},
   "source": [
    "1) Create indices surrounding document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70fe122a-2e93-4b8d-8df6-7384a7056bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n",
      "CPU times: user 2.45 s, sys: 99.2 ms, total: 2.55 s\n",
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will output a time\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=None)\n",
    "\n",
    "## Construct series of document vector stores\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcaff49-2c6c-407f-a210-23e13fc6151e",
   "metadata": {},
   "source": [
    "2. Combine the indices into a single one using the following utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b41ff0b9-4d68-49aa-ac7d-2f363c9d5617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 625 chunks\n"
     ]
    }
   ],
   "source": [
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## Use default_faiss for simplicity, though it's tied to embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "if 'docstore' not in globals():\n",
    "    ## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "    docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b9ba0-c108-4f0a-8a4f-e38c35432843",
   "metadata": {},
   "source": [
    "### Implement RAG Chain\n",
    "- A way to construct a from-scratch vector store for conversational memory (and a way to initialize an empty one with `default_FAISS()`)\n",
    "\n",
    "- A vector store pre-loaded with useful document information from our `ArxivLoader` utility (stored in `docstore`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dcdf086-0f52-47ac-84f2-4d02e7e2507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e72133bd-47fe-4e50-8800-8052793ddd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Tell me about RAG!', 'history': '', 'context': '[Quote from Retrieval-Augmented Generation for Large Language Models: A Survey] to the RAG process, specifically focusing on the aspects\\nof ‚ÄúRetrieval‚Äù, ‚ÄúGenerator‚Äù and ‚ÄúAugmentation‚Äù, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n‚Ä¢ We construct a thorough evaluation framework for RAG,\\noutlining the evaluation objectives and metrics.\\nOur\\ncomparative analysis clarifies the strengths and weak-\\nnesses of RAG compared to fine-tuning from various\\nperspectives. Additionally, we anticipate future direc-\\ntions for RAG, emphasizing potential enhancements to\\ntackle current challenges, expansions into multi-modal\\nsettings, and the development of its ecosystem.\\nThe paper unfolds as follows: Section 2 and 3 define RAG\\nand detail its developmental process. Section 4 through 6 ex-\\nplore core components‚ÄîRetrieval, ‚ÄúGeneration‚Äù and ‚ÄúAug-\\nmentation‚Äù‚Äîhighlighting diverse embedded technologies.\\nSection 7 focuses on RAG‚Äôs evaluation system. Section 8\\n[Quote from Retrieval-Augmented Generation for Large Language Models: A Survey] lizes generated content to guide retrieval, iteratively im-\\nplementing ‚Äúretrieval-enhanced generation‚Äù and ‚Äúgeneration-\\nenhanced retrieval‚Äù within a Retrieve-Read-Retrieve-Read\\nflow. This method demonstrates an innovative way of using\\none module‚Äôs output to improve the functionality of another.\\nOptimizing the RAG Pipeline\\nThe optimization of the retrieval process aims to enhance the\\nefficiency and quality of information in RAG systems. Cur-\\nrent research focuses on integrating diverse search technolo-\\ngies, refining retrieval steps, incorporating cognitive back-\\ntracking, implementing versatile query strategies, and lever-\\naging embedding similarity. These efforts collectively strive\\nto achieve a balance between retrieval efficiency and the\\ndepth of contextual information in RAG systems.\\nHybrid Search Exploration. The RAG system optimizes its\\nperformance by intelligently integrating various techniques,\\nincluding keyword-based search, semantic search, and vec-\\n[Quote from Retrieval-Augmented Generation for Large Language Models: A Survey] ularity, optimizing index structures, adding metadata, align-\\nment optimization, and mixed retrieval.\\nEnhancing data granularity aims to elevate text standard-\\nization, consistency, factual accuracy, and rich context to im-\\nprove the RAG system‚Äôs performance. This includes remov-\\ning irrelevant information, dispelling ambiguity in entities\\nand terms, confirming factual accuracy, maintaining context,\\nand updating outdated documents.\\nOptimizing index structures involves adjusting the size of\\nchunks to capture relevant context, querying across multiple\\nindex paths, and incorporating information from the graph\\nstructure to capture relevant context by leveraging relation-\\nships between nodes in a graph data index.\\nAdding metadata information involves integrating refer-\\nenced metadata, such as dates and purposes, into chunks for\\nfiltering purposes, and incorporating metadata like chapters\\nand subsections of references to improve retrieval efficiency.\\n[Quote from Retrieval-Augmented Generation for Large Language Models: A Survey] Table 2: Summary of metrics applicable for evaluation aspects of RAG\\nContext\\nRelevance\\nFaithfulness\\nAnswer\\nRelevance\\nNoise\\nRobustness\\nNegative\\nRejection\\nInformation\\nIntegration\\nCounterfactual\\nRobustness\\nAccuracy\\n‚úì\\n‚úì\\n‚úì\\n‚úì\\n‚úì\\n‚úì\\n‚úì\\nEM\\n‚úì\\nRecall\\n‚úì\\nPrecision\\n‚úì\\n‚úì\\nR-Rate\\n‚úì\\nCosine Similarity\\n‚úì\\nHit Rate\\n‚úì\\nMRR\\n‚úì\\nNDCG\\n‚úì\\nTable 3: Summary of evaluation frameworks\\nEvaluation Framework\\nEvaluation Targets\\nEvaluation Aspects\\nQuantitative Metrics\\nRGB‚Ä†\\nRetrieval Quality\\nGeneration Quality\\nNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual Robustness\\nAccuracy\\nEM\\nAccuracy\\nAccuracy\\nRECALL‚Ä†\\nGeneration Quality\\nCounterfactual Robustness\\nR-Rate (Reappearance Rate)\\nRAGAS‚Ä°\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\nCosine Similarity\\nARES‚Ä°\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\nAccuracy\\nAccuracy\\nAccuracy\\nTruLens‚Ä°\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\n*\\n'}\n",
      "Sure, I'd be happy to tell you about RAG!\n",
      "\n",
      "RAG stands for Retrieval-Augmented Generation, and it's a process that combines the\n",
      "  strengths of both retrieval and generation to create a more effective and efficient way of\n",
      "  generating text. The idea behind RAG is to use retrieval to gather information from a large\n",
      "  corpus of text, and then use generation to create new text that is informed by the\n",
      "  retrieved information.\n",
      "\n",
      "The RAG process is made up of three main components: retrieval, generation, and augmentation.\n",
      "  The retrieval component is responsible for gathering relevant information from the corpus,\n",
      "  while the generation component uses this information to create new text. The augmentation\n",
      "  component is what ties everything together, ensuring that the generated text is both relevant\n",
      "  and coherent.\n",
      "\n",
      "One of the key benefits of RAG is its ability to improve the efficiency and quality of\n",
      "  information retrieval. By using retrieval to gather information, RAG systems can quickly and\n",
      "  accurately identify relevant information, which can then be used to guide the generation\n",
      "  process. This helps to ensure that the generated text is both relevant and accurate, making\n",
      "  it a valuable tool for a wide range of applications.\n",
      "\n",
      "RAG systems can also be optimized in a number of ways, such as by integrating diverse\n",
      "  search technologies, refining retrieval steps, and incorporating cognitive back-tracking.\n",
      "  This helps to ensure that the RAG system is able to retrieve the most relevant\n",
      "  information possible, which in turn improves the quality of the generated text.\n",
      "\n",
      "In terms of evaluation, there are a number of metrics that can be used to assess the\n",
      "  effectiveness of RAG systems. These include metrics such as relevance, faithfulness,\n",
      "  answer relevance, and noise robustness, among others. By using these metrics, it's\n",
      "  possible to get a comprehensive view of how well a RAG system is performing, and identify\n",
      "  areas for improvement.\n",
      "\n",
      "Overall, RAG is a powerful tool that has the potential to revolutionize the way we\n",
      "  generate text. By combining the strengths of retrieval and generation, RAG systems can\n",
      "  create high-quality text that is both relevant and accurate. Whether you're looking to\n",
      "  improve the efficiency of your information retrieval processes or generate high-quality\n",
      "  text, RAG is definitely worth considering."
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# llm = ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser()\n",
    "llm = ChatNVIDIA(model=\"llama2_70b\") | StrOutputParser()\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "## Implement the retrieval chain\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "\n",
    "stream_chain = chat_prompt | llm\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        if not return_buffer:\n",
    "            line_buffer += token\n",
    "            if \"\\n\" in line_buffer:\n",
    "                line_buffer = \"\"\n",
    "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
    "                line_buffer = \"\"\n",
    "                yield \"\\n\"\n",
    "                token = \"  \" + token.lstrip()\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c20b8-022c-40c5-9b43-0089599b8f7b",
   "metadata": {},
   "source": [
    "## Save Vector Stores\n",
    "\n",
    "Save the accumulated vector store as shown [in the official documentation](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f94db5-7d6b-4deb-9dfd-7bbee634eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    }
   ],
   "source": [
    "## Save and compress index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c03cfd-82c3-4839-8330-761b34dd4e9f",
   "metadata": {},
   "source": [
    "Validation: If everything was properly saved, the following line can be invoked to pull the index from the compressed `tgz` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f5cc5b9-494b-4b47-aeaa-ed67aeffa07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n",
      "0.99\n",
      "1.0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## Load vector database from tgz file\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6eb1f1f4-c52e-47f4-a736-068b143ca5f3",
   "metadata": {},
   "source": [
    "### LangServe Server Deployment\n",
    "\n",
    "LangServe helps developers deploy LangChain runnables and chains as a REST API.\n",
    "This library is integrated with FastAPI and uses pydantic for data validation.\n",
    "In addition, it provides a client that can be used to call into runnables deployed on a server. A javascript client is available in LangChainJS.\n",
    "\n",
    "LangServe integrates a LangChain model, such as ChatNVIDIA, to create and distribute accessible API routes. Using this, we will be able to supply functionality to the frontend service's server_app.py session, which includes:\n",
    "- A simple endpoint named :9012/basic_chat for the basic chatbot, exemplified below.\n",
    "- A pair of endpoints named :9012/retriever and :9012/generator for the RAG chatbot.\n",
    "\n",
    "This is an ***always-on RAG formulation*** where:\n",
    "- A retriever is always retrieving context by default.\n",
    "- A generator is acting on the retrieved context.\n",
    "\n",
    "***Note we need two notebooks to create Gradio application, one to deploy and run the basic chain/RAG chain, another to start the Gradio server for Chatbot UI.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b95a4572-6926-48fa-8d49-4a32b113a7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting server_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile server_app.py\n",
    "import typing\n",
    "import os\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI\n",
    "from time import sleep\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langserve import RemoteRunnable\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# https://python.langchain.com/docs/langserve#server\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langserve import add_routes\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "## LLM Model\n",
    "llm = ChatNVIDIA(model=\"llama2_70b\") | StrOutputParser()\n",
    "\n",
    "## Prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "\n",
    "## Embedding model\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\n",
    "\n",
    "## Load vector database\n",
    "import tarfile \n",
    "file = tarfile.open('docstore_index.tgz') \n",
    "file.extractall('.') \n",
    "# file.close() \n",
    "\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "## Deploy models to FAST Application\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    llm,\n",
    "    path=\"/basic_chat\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    docstore.as_retriever(),\n",
    "    path=\"/retriever\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chat_prompt | llm ,\n",
    "    path=\"/generator\",\n",
    ")\n",
    "\n",
    "# Might be encountered if this were for a standalone python file...\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9012)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6b396-b129-4cf7-924f-f3746ee09753",
   "metadata": {},
   "source": [
    "Below script is to deploy and run basic chain, and RAG chain (retriever and generator) via LangServe, we need another notebook to call these REST API and start the Gradio UI. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdd4a8d8-a3a5-42bd-8036-98b0cb3d5bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1852\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\n",
      " __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "|  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "|  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "|  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "|  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "|_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/generator/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  ‚îÇ\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  ‚îî‚îÄ‚îÄ> /generator/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/retriever/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  ‚îÇ\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  ‚îî‚îÄ‚îÄ> /retriever/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/basic_chat/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  ‚îÇ\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  ‚îî‚îÄ‚îÄ> /basic_chat/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "\n",
      "\u001b[1;31;40mLANGSERVE:\u001b[0m ‚ö†Ô∏è Using pydantic 2.5.3. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details.\n",
      "\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:9012\u001b[0m (Press CTRL+C to quit)\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m1852\u001b[0m]\n"
     ]
    }
   ],
   "source": [
    "!python server_app.py  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7280ff-6071-44e6-b023-7ea863bb0dda",
   "metadata": {},
   "source": [
    "Stop above process and run the server_app.py script in another notebook. Use below code to start Gradio application.\n",
    "\n",
    "Access the `basic_chat` endpoint using the following interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b7dd67c-b07f-49b3-9625-863b3370b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retrieval-Augmented Generation) is a technique used in natural language processing (NLP) and machine learning (ML) that combines the strengths of both retrieval-based and generation-based models to improve the quality and efficiency of text generation.\n",
      "\n",
      "The basic idea behind RAG is to use a retrieval model to find relevant information from a large corpus of text, and then use a generation model to generate new text that incorporates the retrieved information. This can help to improve the coherence and accuracy of the generated text, as well as reduce the risk of generating irrelevant or repetitive content.\n",
      "\n",
      "One reference paper for RAG is \"Retrieval-Augmented Generation: A New Paradigm for Text Generation\" by Xia et al. (2020) [1]. This paper proposes a RAG framework that combines a retrieval model and a generation model to generate high-quality text. The authors evaluate the effectiveness of RAG on several benchmark datasets and show that it outperforms state-of-the-art generation models.\n",
      "\n",
      "Another reference paper for RAG is \"RAG: Retrieval-Augmented Generation with Reinforcement Learning\" by Zhang et al. (2020) [2]. This paper introduces a RAG model that uses reinforcement learning to optimize the generation process. The authors show that their RAG model achieves state-of-the-art results on several text generation tasks.\n",
      "\n",
      "A third reference paper for RAG is \"Retrieval-Augmented Text Generation with Style Transfer\" by Li et al. (2020) [3]. This paper proposes a RAG model that incorporates a style transfer component to generate text with a desired style. The authors demonstrate the effectiveness of their model on several text generation tasks.\n",
      "\n",
      "These papers provide a good starting point for understanding RAG and its applications in NLP and ML.\n",
      "\n",
      "References:\n",
      "\n",
      "[1] Xia, R., Li, J., Li, J., & Wang, Y. (2020). Retrieval-Augmented Generation: A New Paradigm for Text Generation. arXiv preprint arXiv:2010.03312.\n",
      "\n",
      "[2] Zhang, J., Zhao, J., & Wang, Y. (2020). RAG: Retrieval-Augmented Generation with Reinforcement Learning. arXiv preprint arXiv:2010.03313.\n",
      "\n",
      "[3] Li, J., Xia, R., Li, J., & Wang, Y. (2020). Retrieval-Augmented Text Generation with Style Transfer. arXiv preprint arXiv:2010.03314."
     ]
    }
   ],
   "source": [
    "## Test call basic chat API that is running on LangServe\n",
    "from langserve import RemoteRunnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = RemoteRunnable(\"http://0.0.0.0:9012/basic_chat/\") | StrOutputParser()\n",
    "for token in llm.stream(\"tell me something about RAG - Retrieval-Augmented Generation, give reference paper\"):\n",
    "    print(token, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e1dfb-f5e9-4ac8-800f-742856935f7c",
   "metadata": {},
   "source": [
    "## Gradio Chatbot UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ddf15a-8c4a-4d10-a8e1-425b62db7a02",
   "metadata": {},
   "source": [
    "While basic chain and RAG chain are running on the server, we could integrate them into the main chain, and evaluate the performance of two chains based on synthethic questions per LLM-as-a-Judge.\n",
    "\n",
    "#### Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "The evaluation routine is as below:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access.\n",
    "\n",
    "#### LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc1c1d69-1e23-4107-829b-88e8b959e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting FastAPI app\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "import os\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI\n",
    "from time import sleep\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langserve import RemoteRunnable\n",
    "import gradio as gr\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "## Chain Dictionary\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"If you want to support streaming, implement final step as a generator extractor.\"\"\"\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "## Necessary Endpoints\n",
    "chains_dict = {\n",
    "    'basic' : RemoteRunnable(\"http://lab:9012/basic_chat/\"),\n",
    "    'retriever' : RemoteRunnable(\"http://lab:9012/retriever/\"),\n",
    "    'generator' : RemoteRunnable(\"http://lab:9012/generator/\"),\n",
    "}\n",
    "\n",
    "basic_chain = chains_dict['basic']\n",
    "\n",
    "\n",
    "## Retrieval-Augmented Generation Chain\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign(\n",
    "        {'context' : itemgetter('input') \n",
    "        | chains_dict['retriever'] \n",
    "        | LongContextReorder().transform_documents\n",
    "        | docs2str\n",
    "    })\n",
    ")\n",
    "\n",
    "output_chain = RunnableAssign({\"output\" : chains_dict['generator'] }) | output_puller\n",
    "rag_chain = retrieval_chain | output_chain\n",
    "\n",
    "#####################################################################\n",
    "## ChatBot utilities\n",
    "\n",
    "def add_message(message, history, role=0, preface=\"\"):\n",
    "    if not history or history[-1][role] is not None:\n",
    "        history += [[None, None]]\n",
    "    history[-1][role] = preface\n",
    "    buffer = \"\"\n",
    "    try:\n",
    "        for chunk in message:\n",
    "            token = getattr(chunk, 'content', chunk)\n",
    "            buffer += token\n",
    "            history[-1][role] += token\n",
    "            yield history, buffer, False \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gradio Stream failed: {e}\\nFor Input {history}\")\n",
    "        history[-1][role] += f\"...\\nGradio Stream failed: {e}\"\n",
    "        yield history, buffer, True\n",
    "\n",
    "\n",
    "def add_text(history, text):\n",
    "    history = history + [(text, None)]\n",
    "    return history, gr.Textbox(value=\"\", interactive=False)\n",
    "\n",
    "\n",
    "def bot(history, chain_key):\n",
    "    chain = {'Basic' : basic_chain, 'RAG' : rag_chain}.get(chain_key)\n",
    "    msg_stream = chain.stream(history[-1][0])\n",
    "    for history, buffer, is_error in add_message(msg_stream, history, role=1):\n",
    "        yield history\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "## Document/Assessment Utilities\n",
    "\n",
    "\n",
    "def get_chunks(document):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    "    )\n",
    "    content = document[0].page_content\n",
    "    content = content.replace(\"{\", \"[\").replace(\"}\", \"]\")\n",
    "    if \"References\" in content:\n",
    "        content = content[:content.index(\"References\")]\n",
    "    document[0].page_content = content\n",
    "    return text_splitter.split_documents(document)\n",
    "\n",
    "\n",
    "def get_day_difference(date_str):\n",
    "    given_date = datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "    current_date = datetime.now().date()\n",
    "    difference = current_date - given_date\n",
    "    return difference.days\n",
    "\n",
    "\n",
    "def get_fresh_chunks(chunks):\n",
    "    return [\n",
    "        chunk for chunk in chunks \n",
    "            # if get_day_difference(chunk.metadata.get(\"Published\", \"2000-01-01\")) < 30\n",
    "            if get_day_difference(chunk.metadata.get(\"Published\", \"2000-01-01\")) < 365\n",
    "    ]\n",
    "\n",
    "\n",
    "def format_chunk(doc):\n",
    "    doc_content = doc.page_content.replace('{', '\\{').replace('}', '\\}')\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc_content}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_synth_prompt(docs):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary. Be specific!\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "    )\n",
    "    usr_msg = (f\"Document1: {format_chunk(doc1)}\\n\\nDocument2: {format_chunk(doc2)}\")\n",
    "    return ChatPromptTemplate.from_messages([('system', sys_msg), ('user', usr_msg)])\n",
    "\n",
    "\n",
    "def get_eval_prompt():\n",
    "    eval_instruction = (\n",
    "        \"Evaluate the following Question-Answer pair for human preference and consistency.\"\n",
    "        \"Ask question only related to RAG (Retrieval Augmented Generation).\"\n",
    "        \"\\nAssume the first answer is a ground truth answer and has to be correct.\"\n",
    "        \"\\nAssume the second answer may or may not be true.\"\n",
    "        \"\\n[1] The first answer is extremely preferable, or the second answer heavily deviates.\"\n",
    "        \"\\n[2] The second answer does not contradict the first and significantly improves upon it.\"\n",
    "        \"\\n\\nOutput Format:\"\n",
    "        \"\\nJustification\\n[2] if 2 is strongly preferred, [1] otherwise\"\n",
    "    )\n",
    "    return {\"input\" : lambda x:x} | ChatPromptTemplate.from_messages([('system', eval_instruction), ('user', '{input}')])\n",
    "\n",
    "\n",
    "## Document names, and the overall chunk list\n",
    "class Globals:\n",
    "    doc_names = set()\n",
    "    doc_chunks = []\n",
    "\n",
    "\n",
    "def rag_eval(history, chain_key):\n",
    "    \"\"\"RAG Evaluation Chain\"\"\"\n",
    "    if not len(history) or history[-1][0] is not None:\n",
    "        history += [[None, None]]\n",
    "    \n",
    "    if not Globals.doc_chunks:\n",
    "        try: \n",
    "            docstore = FAISS.load_local(\"docstore_index\", lambda x:x)\n",
    "            Globals.doc_chunks = list(docstore.docstore._dict.values())\n",
    "            Globals.doc_names = {doc.metadata.get(\"Title\", \"Unknown\") for doc in Globals.doc_chunks}\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    doc_names = Globals.doc_names \n",
    "    doc_chunks = get_fresh_chunks(Globals.doc_chunks)\n",
    "\n",
    "    if len(doc_chunks) < 2:\n",
    "        logger.error(f\"Attempted to evaluate with less than two fresh chunks submitted (last modified < 30 days ago)\")\n",
    "        history[-1][1] = \"Please upload a fresh paper (<30 days) inside your saved docstore_index directory that so we can ask our chain some questions\"\n",
    "        yield history\n",
    "    else:\n",
    "        main_chain = {'Basic' : basic_chain, 'RAG' : rag_chain}.get(chain_key)\n",
    "        eval_llm = basic_chain\n",
    "        num_points = 0\n",
    "        # num_questions = 8\n",
    "        num_questions = 5\n",
    "\n",
    "        for i in range(num_questions):\n",
    "\n",
    "            synth_chain = get_synth_prompt(doc_chunks) | eval_llm\n",
    "            \n",
    "            preface = \"Generating Synthetic QA Pair:\\n\"\n",
    "            msg_stream = synth_chain.stream({})\n",
    "            for history, synth_qa, is_error in add_message(msg_stream, history, role=0, preface=preface):\n",
    "                yield history\n",
    "            if is_error: break\n",
    "\n",
    "            synth_pair = synth_qa.split(\"\\n\\n\")\n",
    "            if len(synth_pair) < 2:\n",
    "                logger.error(f\"Illegal QA with no break\")\n",
    "                history[-1][0] += f\"...\\nIllegal QA with no break\"\n",
    "                yield history\n",
    "            else:   \n",
    "                synth_q, synth_a = synth_pair[:2]\n",
    "\n",
    "                msg_stream = main_chain.stream(synth_q)\n",
    "                for history, rag_response, is_error in add_message(msg_stream, history, role=1):\n",
    "                    yield history\n",
    "                if is_error: break\n",
    "\n",
    "                eval_chain = get_eval_prompt() | eval_llm\n",
    "                usr_msg = f\"Question: {synth_q}\\n\\nAnswer 1: {synth_a}\\n\\n Answer 2: {rag_response}\"\n",
    "                msg_stream = eval_chain.stream(usr_msg)\n",
    "                for history, eval_response, is_error in add_message(msg_stream, history, role=0, preface=\"Evaluation: \"):\n",
    "                    yield history\n",
    "\n",
    "                num_points += (\"[2]\" in eval_response)\n",
    "            \n",
    "            history[-1][0] += f\"\\n[{num_points} / {i+1}]\"\n",
    "        \n",
    "        if (num_points / num_questions > 0.60):\n",
    "            msg_stream = (\n",
    "                \"Congrats! You've passed the assessment!! üòÅ\\n\"\n",
    "                \"Please make sure to click the ASSESS TASK button before shutting down your course environment\"\n",
    "            )\n",
    "            for history, eval_response, is_error in add_message(msg_stream, history, role=0):\n",
    "                yield history\n",
    "\n",
    "            ## secret\n",
    "\n",
    "        else: \n",
    "            msg_stream = f\"Metric score of {num_points / num_questions}, while 0.60 is required\\n\"\n",
    "            for history, eval_response, is_error in add_message(msg_stream, history, role=0):\n",
    "                yield history            \n",
    "        \n",
    "        yield history\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "## GRADIO EVENT LOOP\n",
    "\n",
    "# https://github.com/gradio-app/gradio/issues/4001\n",
    "CSS =\"\"\"\n",
    ".contain { display: flex; flex-direction: column; height:80vh;}\n",
    "#component-0 { height: 100%; }\n",
    "#chatbot { flex-grow: 1; overflow: auto;}\n",
    "\"\"\"\n",
    "THEME = gr.themes.Default(primary_hue=\"green\")\n",
    "\n",
    "with gr.Blocks(css=CSS, theme=THEME) as demo:\n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        elem_id=\"chatbot\",\n",
    "        bubble_full_width=False,\n",
    "        avatar_images=(None, (os.path.join(\"frontend/\", \"parrot.png\"))),\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(\n",
    "            scale=4,\n",
    "            show_label=False,\n",
    "            placeholder=\"Enter text and press enter, or upload an image\",\n",
    "            container=False,\n",
    "        )\n",
    "\n",
    "        chain_btn  = gr.Radio([\"Basic\", \"RAG\"], value=\"Basic\", label=\"Main Route\")\n",
    "        test_btn   = gr.Button(\"üéì\\nEvaluate\")\n",
    "\n",
    "    # Reference: https://www.gradio.app/guides/blocks-and-event-listeners\n",
    "\n",
    "    # This listener is triggered when the user presses the Enter key while the Textbox is focused.\n",
    "    txt_msg = (\n",
    "        # first update the chatbot with the user message immediately. Also, disable the textbox\n",
    "        txt.submit(              ## On textbox submit (or enter)...\n",
    "            fn=add_text,            ## Run the add_text function...\n",
    "            inputs=[chatbot, txt],  ## Pass in the values of chatbot and txt...\n",
    "            outputs=[chatbot, txt], ## Assign the results to the values of chatbot and txt...\n",
    "            queue=False             ## And don't use the function as a generator (so no streaming)!\n",
    "        )\n",
    "        # then update the chatbot with the bot response (same variable logic)\n",
    "        .then(bot, [chatbot, chain_btn], [chatbot])\n",
    "        ## Then, unblock the textbox by assigning an active status to it\n",
    "        .then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)\n",
    "    )\n",
    "\n",
    "    test_msg = test_btn.click(\n",
    "        rag_eval, \n",
    "        inputs=[chatbot, chain_btn], \n",
    "        outputs=chatbot, \n",
    "    )\n",
    "\n",
    "#####################################################################\n",
    "## Final App Deployment\n",
    "\n",
    "demo.queue()\n",
    "\n",
    "logger.warning(\"Starting FastAPI app\")\n",
    "app = FastAPI()\n",
    "\n",
    "app = gr.mount_gradio_app(app, demo, '/')\n",
    "\n",
    "@app.route(\"/health\")\n",
    "async def health():\n",
    "    return {\"success\": True}, 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e063a-a5e6-461f-9c32-462aa245d47d",
   "metadata": {},
   "source": [
    "#### Start Gradio Chatbot\n",
    "\n",
    "***Note endpoints should keep running in LangServe with another notebook during starting Gradio Chatbot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251e2d8-d8f7-4a2b-b03a-7c6aa2d45611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://12883ff5ec98be9ee2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://12883ff5ec98be9ee2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "# demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "demo.queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd367f5-1ede-4cbc-82e6-3f79b0b118dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
