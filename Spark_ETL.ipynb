{
  "metadata": {
    "name": "Survey_Data_ETL",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pyspark\nfrom pyspark.context import SparkContext\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import row_number, expr, explode, udf, col,  monotonically_increasing_id, lit\nfrom pyspark.sql.types import StructType, IntegerType, StringType, ByteType\nfrom pyspark.sql.window import Window\nimport datetime, time\nimport random\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import types as T"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsample \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/1025819546169638082/impression_log/593353618315725011/2022/11/16/part-00000-bd696fa9-e02d-42c1-91f1-e86dde4c27ac-c000.snappy.parquet\")\nsample.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsample.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# SD Impression\nsd_impression \u003d spark.sql(\"\"\"\nselect *\nfrom spektr_ach.d_sd_ad_impressions\nwhere region_id \u003d 1 and spektr_date \u003e \u00272023-01-01\u0027 and marketplace_id \u003d 1 and viewed \u003d 1\n\"\"\")\nsd_impression.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsd_impression.filter(F.col(\"spektr_date\")\u003c\u003d\"2022-11-23\").agg(F.count(\u0027*\u0027)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsd_impression.createOrReplaceTempView(\"sd_impression\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"select distinct total_impressions, valid_impressions, invalid_impressions from sd_impression\"\"\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsd_impression.select([\"total_impressions\",\"valid_impressions\",\"invalid_impressions\"]).dropDuplicates().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# ASIN brand mapping\nd_mp_asins_snapshot \u003d spark.sql(\"\"\"\nSELECT asin, brand_id, merchant_brand_name\nFROM spektr_booker.d_mp_asins_snapshot\nWHERE region_id \u003d 1 AND marketplace_id \u003d 1 AND spektr_date \u003d \u00272022-11-20\u0027\nAND merchant_brand_name IN (\u0027Nilight\u0027, \u0027OnePlus\u0027, \u0027Columbia\u0027, \u0027Rainbocorns\u0027, \u0027Kingston\u0027, \u0027Utopia Kitchen\u0027, \u00273Doodler\u0027, \u0027Arctix\u0027, \u0027Catalonia\u0027, \u0027Panda Grip\u0027)\n\"\"\").dropDuplicates()\n\nd_mp_asins_snapshot.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nd_mp_asins_snapshot.select(\"merchant_brand_name\").distinct().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nd_mp_asins_snapshot.createOrReplaceTempView(\"d_mp_asins_snapshot\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"select merchant_brand_name \n             from d_mp_asins_snapshot \n             where upper(merchant_brand_name) like \u0027%NILIGHT%\u0027 \n                or upper(merchant_brand_name) like \u0027%ONEPLUS%\u0027\n                or upper(merchant_brand_name) like \u0027%COLUMBIA%\u0027\n                or upper(merchant_brand_name) like \u0027%RAINBOCORNS%\u0027\n                or upper(merchant_brand_name) like \u0027%KINGSTON%\u0027\n                or upper(merchant_brand_name) like \u0027%UTOPIA KITCHEN%\u0027\n                or upper(merchant_brand_name) like \u0027%3DOODLER%\u0027\n                or upper(merchant_brand_name) like \u0027%ARCTIX%\u0027\n                or upper(merchant_brand_name) like \u0027%CATALONIA%\u0027\n                or upper(merchant_brand_name) like \u0027%PANDAS GRIP%\u0027 \n            group by 1\n                \"\"\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n//  d_mp_asins_snapshot.select(\"merchant_brand_name\").distinct().repartition(1).write.option(\"header\", True).csv(\"s3u://fortunax/survey-analysis/brand_name\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# method 1\n# join sd impressoin with asin to bring in brand name\nbrand_imp \u003d sd_impression.select(\"customer_id\", F.col(\"asin\"), \"campaign_id\",\"total_impressions\").join(d_mp_asins_snapshot, \"asin\", \"inner\")\n\n#  brand_imp.repartition(1).write.option(\"header\", True).csv(\"s3u://fortunax/survey-analysis/sd_survey_imp/\")\n\nbrand_imp.repartition(10, \"merchant_brand_name\").write.mode(\u0027overwrite\u0027).partitionBy(\"merchant_brand_name\").parquet(\"s3u://fortunax/survey-analysis/sd_survey_imp/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# method 2\n# sd_impression \u003d sd_impression.withColumn(\"asin_sd\", F.col(\"asin\"))\n# brand_imp \u003d sd_impression.select(\"customer_id\", \"asin_sd\", \"campaign_id\",\"total_impressions\").join(d_mp_asins_snapshot, sd_impression.asin_sd \u003d\u003d d_mp_asins_snapshot.asin, \"inner\")\\\n#             .drop(\"asin_sd\")\n\n# #  brand_imp.repartition(1).write.option(\"header\", True).csv(\"s3u://fortunax/survey-analysis/sd_survey_imp/\")\n\n# # write temp folder for complete output\n# brand_imp.repartition(10, \"merchant_brand_name\").write.mode(\u0027overwrite\u0027).partitionBy(\"merchant_brand_name\").parquet(\"s3u://fortunax/survey-analysis/sd_survey_imp/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbrand_imp_new \u003d spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(\"s3u://fortunax/survey-analysis/sd_survey_imp/\")\nbrand_imp_new.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# validation of impressions to previous period\nz.show(brand_imp_new.groupby(\"merchant_brand_name\").agg(F.sum(\"total_impressions\"), F.countDistinct(\"customer_id\"),  F.countDistinct(\"campaign_id\"), F.countDistinct(\"brand_id\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# create campaign id dummary data\ncampaignid_dummy \u003d spark.createDataFrame([\n               Row(merchant_brand_name\u003d\"Nilight\", campaignId\u003d\"7777777777777777001\"),\n               Row(merchant_brand_name\u003d\"OnePlus\", campaignId\u003d\"7777777777777777002\"),\n               Row(merchant_brand_name\u003d\"Columbia\", campaignId\u003d\"7777777777777777003\"),\n               Row(merchant_brand_name\u003d\"Rainbocorns\", campaignId\u003d\"7777777777777777004\"),\n               Row(merchant_brand_name\u003d\"Kingston\", campaignId\u003d\"7777777777777777005\"),\n               Row(merchant_brand_name\u003d\"Utopia Kitchen\", campaignId\u003d\"7777777777777777006\"),\n               Row(merchant_brand_name\u003d\"3Doodler\", campaignId\u003d\"7777777777777777007\"),\n               Row(merchant_brand_name\u003d\"Arctix\", campaignId\u003d\"7777777777777777008\"),\n               Row(merchant_brand_name\u003d\"Catalonia\", campaignId\u003d\"7777777777777777009\"),\n               Row(merchant_brand_name\u003d\"Panda Grip\", campaignId\u003d\"7777777777777777010\")\n               ])\n\ncampaignid_dummy.show()\ncampaignid_dummy.printSchema()\n   "
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncampaignid_dummy.createOrReplaceTempView(\"campaignid_dummy\")"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# spark.sql(\"\"\"SET spark.sql.shuffle.partitions \u003d 2\"\"\")\nsqlContext.setConf(\"spark.sql.shuffle.partitions\", \"1000\")\nsqlContext.setConf(\"spark.sql.adaptive.enabled\", \"true\")\nsqlContext.setConf(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n\n# # or\n# spark.sql(\"\"\"SET spark.sql.shuffle.partitions \u003d 1000\"\"\")\n# spark.sql(\"\"\"SET spark.sql.adaptive.enabled \u003d true\"\"\")\n# spark.sql(\"\"\"SET spark.sql.adaptive.coalescePartitions.enabled \u003d true\"\"\")\n\ncampaignid_dummy \u003d spark.sql(\"\"\"select * from campaignid_dummy CLUSTER BY merchant_brand_name\"\"\")\ncampaignid_dummy.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# create final data\nstart_time \u003d time.time()\n\nfinal_brand \u003d brand_imp_new.select(F.col(\"customer_id\").alias(\"AD_USER_ID\").cast(\"string\"), F.col(\"merchant_brand_name\").cast(\"string\")).distinct()\\\n                           .withColumn(\"adProductTypeCode\",lit(None).cast(\"string\"))\\\n                           .withColumn(\"campaignAttributionAlgoId\",lit(None).cast(\"int\"))\\\n                           .withColumn(\"campaignCfId\", lit(None).cast(\"long\"))\\\n                           .withColumn(\"campaignDimId\", lit(None).cast(\"long\"))\\\n                           .withColumn(\"demandChannelCode\", lit(None).cast(\"string\"))\\\n                           .withColumn(\"demandChannelDimId\", lit(None).cast(\"int\"))\\\n                           .withColumn(\"deviceType\", lit(None).cast(\"string\"))\\\n                           .withColumn(\"siteName\", lit(None).cast(\"string\"))\\\n                           .drop(\"campaign_id\")\\\n                           .join(campaignid_dummy.hint(\"broadcast\"), \"merchant_brand_name\", \"left\")\\\n                           .select(\"merchant_brand_name\",\"AD_USER_ID\",\"adProductTypeCode\",\"campaignAttributionAlgoId\",\"campaignCfId\",\"campaignDimId\",F.col(\"campaignId\").cast(\"long\"),\\\n                                   \"demandChannelCode\",\"demandChannelDimId\", \"deviceType\", \"siteName\")\n\nfinal_brand.show()\n\nend_time \u003d time.time()\nprint(\"Total time for join {}\".format(end_time - start_time))\n\nfinal_brand.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_brand.explain()"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# create final data\nstart_time \u003d time.time()\n\nfinal_brand \u003d brand_imp_new.select(F.col(\"customer_id\").alias(\"AD_USER_ID\").cast(\"string\"), F.col(\"merchant_brand_name\").cast(\"string\")).distinct()\\\n                           .withColumn(\"adProductTypeCode\",lit(None).cast(\"string\"))\\\n                           .withColumn(\"campaignAttributionAlgoId\",lit(None).cast(\"int\"))\\\n                           .withColumn(\"campaignCfId\", lit(None).cast(\"long\"))\\\n                           .withColumn(\"campaignDimId\", lit(None).cast(\"long\"))\\\n                           .withColumn(\"demandChannelCode\", lit(None).cast(\"string\"))\\\n                           .withColumn(\"demandChannelDimId\", lit(None).cast(\"int\"))\\\n                           .withColumn(\"deviceType\", lit(None).cast(\"string\"))\\\n                           .withColumn(\"siteName\", lit(None).cast(\"string\"))\\\n                           .drop(\"campaign_id\")\\\n                           .join(campaignid_dummy, \"merchant_brand_name\", \"left\")\\\n                           .select(\"merchant_brand_name\",\"AD_USER_ID\",\"adProductTypeCode\",\"campaignAttributionAlgoId\",\"campaignCfId\",\"campaignDimId\",F.col(\"campaignId\").cast(\"long\"),\\\n                                   \"demandChannelCode\",\"demandChannelDimId\", \"deviceType\", \"siteName\")\n\nfinal_brand.show()\n\nend_time \u003d time.time()\nprint(\"Total time for join {}\".format(end_time - start_time))\n\nfinal_brand.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_brand.explain()"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_brand.explain()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"SET spark.sql.adaptive.enabled \u003d true\"\"\")\n# create final data\nstart_time \u003d time.time()\n\nfinal_brand \u003d brand_imp_new.select(F.col(\"customer_id\").alias(\"AD_USER_ID\").cast(\"string\"), F.col(\"merchant_brand_name\").cast(\"string\")).distinct()\\\n                           .withColumn(\"adProductTypeCode\",lit(None).cast(\"string\"))\\\n                           .withColumn(\"campaignAttributionAlgoId\",lit(None).cast(\"int\"))\\\n                           .withColumn(\"campaignCfId\", lit(None).cast(\"long\"))\\\n                           .withColumn(\"campaignDimId\", lit(None).cast(\"long\"))\\\n                           .withColumn(\"demandChannelCode\", lit(None).cast(\"string\"))\\\n                           .withColumn(\"demandChannelDimId\", lit(None).cast(\"int\"))\\\n                           .withColumn(\"deviceType\", lit(None).cast(\"string\"))\\\n                           .withColumn(\"siteName\", lit(None).cast(\"string\"))\\\n                           .drop(\"campaign_id\")\\\n                           .join(campaignid_dummy, \"merchant_brand_name\", \"left\")\\\n                           .select(\"merchant_brand_name\",\"AD_USER_ID\",\"adProductTypeCode\",\"campaignAttributionAlgoId\",\"campaignCfId\",\"campaignDimId\",F.col(\"campaignId\").cast(\"long\"),\\\n                                   \"demandChannelCode\",\"demandChannelDimId\", \"deviceType\", \"siteName\")\n\nfinal_brand.show()\n\nend_time \u003d time.time()\nprint(\"Total time for join {}\".format(end_time - start_time))\n\nfinal_brand.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nz.show(final_brand.sort(\"campaignId\").groupby(\"merchant_brand_name\",\"campaignId\").agg(F.count(\u0027*\u0027)))"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# write to s3 partition by survey id and campaign id\nNilight \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"Nilight\").drop(\"merchant_brand_name\")\nNilight.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/65083402-f90f-4400-b673-d56dcbd53688/impression_log/7777777777777001/2022/11/23/\")\nOnePlus \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"OnePlus\").drop(\"merchant_brand_name\")\nOnePlus.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/948b9355-445a-4930-b696-c1e336080e4b/impression_log/7777777777777002/2022/11/23/\")\nColumbia \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"Columbia\").drop(\"merchant_brand_name\")\nColumbia.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/eed00fe9-f36c-4e04-8396-2e1a523bfa27/impression_log/7777777777777003/2022/11/23/\")\nRainbocorns \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"Rainbocorns\").drop(\"merchant_brand_name\")\nRainbocorns.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/11c393ba-76c9-42f3-b07e-eaef14572c1f/impression_log/7777777777777004/2022/11/23/\")\nKingston \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"Kingston\").drop(\"merchant_brand_name\")\nKingston.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/762a62fe-f204-414e-8a75-66418488758a/impression_log/7777777777777005/2022/11/23/\")\nUtopiaKitchen \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"Utopia Kitchen\").drop(\"merchant_brand_name\")\nUtopiaKitchen.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/c0980a5a-867e-4c1f-a1cd-a2a3efaeff68/impression_log/7777777777777006/2022/11/23/\")\nDoodler \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"3Doodler\").drop(\"merchant_brand_name\")\nDoodler.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/ac811bb5-7c9a-47c2-80af-1d2e9ec61f3b/impression_log/7777777777777007/2022/11/23/\")\nArctix \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"Arctix\").drop(\"merchant_brand_name\")\nArctix.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/bf789156-a9f4-49d8-8e61-e9b6c9beecfe/impression_log/7777777777777008/2022/11/23/\")\nCatalonia \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"Catalonia\").drop(\"merchant_brand_name\")\nCatalonia.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/39ca59c6-4467-4209-b848-1d820443750b/impression_log/7777777777777009/2022/11/23/\")\nPandaGrip \u003d final_brand.filter(F.col(\"merchant_brand_name\") \u003d\u003d \"Panda Grip\").drop(\"merchant_brand_name\")\nPandaGrip.repartition(1, \"campaignId\").write.mode(\u0027overwrite\u0027).parquet(\"s3u://audience-generation-output-prod-na/fde0fa22-2b04-49e7-9e5a-6c73b93ea5c2/impression_log/7777777777777010/2022/11/23/\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# final check writing result\nfinal_brand_check \u003d spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(\"s3u://audience-generation-output-prod-na/fde0fa22-2b04-49e7-9e5a-6c73b93ea5c2/impression_log/7777777777777010/2022/11/23/\")\n\nfinal_brand_check.printSchema()\nfinal_brand_check.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_brand_check.groupby(\"campaignId\").agg(F.count(\u0027*\u0027)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# SD Impression\nsd_impression \u003d spark.sql(\"\"\"\nselect *\nfrom spektr_ach.d_sd_ad_impressions\nwhere region_id \u003d 1 and spektr_date \u003e\u003d \u00272023-01-01\u0027 and marketplace_id \u003d 1 and viewed \u003d 1\n\"\"\")\nsd_impression.printSchema()\n\nsd_impression.createOrReplaceTempView(\"sd_impression\")\n\nsd_impression.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# ASIN brand mapping\nd_mp_asins_snapshot \u003d spark.sql(\"\"\"\nSELECT asin, brand_id, merchant_brand_name\nFROM spektr_booker.d_mp_asins_snapshot\nWHERE region_id \u003d 1 AND marketplace_id \u003d 1 AND spektr_date \u003d \u00272023-03-15\u0027\n\"\"\").dropDuplicates()\n\nd_mp_asins_snapshot.createOrReplaceTempView(\"d_mp_asins_snapshot\")\n\nd_mp_asins_snapshot.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsqlContext.setConf(\"spark.driver.maxResultSize\",\"100g\")\nsqlContext.setConf(\"spark.sql.shuffle.partitions\", \"2\")\nsqlContext.setConf(\"spark.sql.adaptive.enabled\", \"false\")\nsqlContext.setConf(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n\n\n# join sd impressoin with asin to bring in brand name\n# top_sd_imp \u003d  sd_impression.select(\"asin\", \"gl_product_group\",\"total_impressions\",\"total_clicks\",\"total_cost\")\\\n#                           .groupBy(\"asin\", \"gl_product_group\")\\\n#                           .agg(F.sum(\"total_impressions\").alias(\"total_impressions\"), F.sum(\"total_clicks\").alias(\"total_clicks\"), F.sum(\"total_cost\").alias(\"total_cost\")).orderBy(F.col(\"total_impressions\").desc()).limit(300)\n\ntop_sd_imp \u003d spark.sql(\"\"\"select asin, gl_product_group, sum(total_impressions) as total_impressions, sum(total_clicks) as total_clicks, sum(total_cost) as total_cost \n                          from sd_impression \n                          group by  asin, gl_product_group\n                          order by total_impressions desc\n                          limit 300 \"\"\")\n                          \ntop_sd_imp.createOrReplaceTempView(\"top_sd_imp\")\ntop_sd_imp.show()\n\n# top_asin \u003d spark.sql(\"\"\"select asin, merchant_brand_name, brand_id from d_mp_asins_snapshot where asin in (select asin from top_sd_imp)\"\"\")\n\n# top_brand_imp \u003d top_sd_imp.select(\"asin\",\"gl_product_group\",\"total_impressions\",\"total_clicks\",\"total_cost\").join(top_asin, \"asin\", \"inner\")\\\n#                          .groupBy(\"merchant_brand_name\",\"brand_id\",\"gl_product_group\").agg(F.sum(\"total_impressions\").alias(\"total_impressions\"), F.sum(\"total_clicks\").alias(\"total_clicks\"), F.sum(\"total_cost\").alias(\"total_cost\"))\\\n#                          .orderBy(F.col(\"total_impressions\").desc())\n# top_brand_imp.show(300, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsd_model_input \u003d  spark.read.option(\"header\", True).csv(\"s3://fortunax/survey-analysis/sd_pilot_result/psm_model_input/model_input_agg.csv\")\nsd_model_input.printSchema()\nsd_model_input.show(truncate \u003d False)\nsd_results \u003d  spark.read.option(\"header\", True).csv(\"s3://fortunax/survey-analysis/sd_pilot_result/data_agg.csv\")\nsd_results.printSchema()\nsd_results.show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsd_results_join \u003d sd_results.select(\"customer_id\",\"Brand\",\"Type\").distinct().withColumnRenamed(\"Brand\",\"Brand_new\")\nsd_final \u003d sd_model_input.drop(F.col(\"_c0\")).join(sd_results_join,\n           (sd_model_input.customer_decoration_key \u003d\u003d sd_results_join.customer_id) \u0026 (sd_model_input.Brand \u003d\u003d sd_results_join.Brand_new), how \u003d \"left\")\\\n           .drop(\"customer_id\",\"Brand_new\")\nsd_final.printSchema()\nsd_final.repartition(1).write.mode(\u0027overwrite\u0027).parquet(\"s3u://fortunax/survey-analysis/sd_pilot_result/model_input_results_agg/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsd_results_join \u003d sd_results.select(\"customer_id\",\"Brand\",\"Type\").distinct().withColumnRenamed(\"Brand\",\"Brand_new\")\nsd_final \u003d sd_model_input.drop(F.col(\"_c0\")).join(sd_results_join,\n           (sd_model_input.customer_decoration_key \u003d\u003d sd_results_join.customer_id) \u0026 (sd_model_input.Brand \u003d\u003d sd_results_join.Brand_new), how \u003d \"inner\")\\\n           .drop(\"customer_id\",\"Brand_new\")\nsd_final.printSchema()\nsd_final.repartition(1).write.mode(\u0027overwrite\u0027).parquet(\"s3u://fortunax/survey-analysis/sd_pilot_result/model_input_results_only_agg/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmodeldata_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/65083402-f90f-4400-b673-d56dcbd53688/65083402-f90f-4400-b673-d56dcbd53688/65083402-f90f-4400-b673-d56dcbd53688/model_data_IPW_final_launch/*\")\nmodeldata_results.printSchema()\nmodeldata_results.show(10)\nmodeldata_results.createOrReplaceTempView(\"modeldata_results\")"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# write model data to consolidated s3 bucket\nbrand \u003d \u0027Catalonia\u0027\nmodeldata_results.repartition(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"s3u://fortunax/survey-analysis/sd_pilot_result/psm_model_input/\"+brand+\"_psm_model_input\")"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmodel_input_agg \u003d spark.read.csv(\"s3://fortunax/survey-analysis/sd_pilot_result/psm_model_input/model_input_agg.csv\")\nmodel_input_agg.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeature_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/65083402-f90f-4400-b673-d56dcbd53688/65083402-f90f-4400-b673-d56dcbd53688/65083402-f90f-4400-b673-d56dcbd53688/featurized_data_IPW_final_launch/*\")\nfeature_results.printSchema()\nfeature_results.show(10)\nfeature_results.createOrReplaceTempView(\"feature_results\")"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprojection_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/65083402-f90f-4400-b673-d56dcbd53688/65083402-f90f-4400-b673-d56dcbd53688/65083402-f90f-4400-b673-d56dcbd53688/projection_IPW_final_launch/*\")\nprojection_results.printSchema()\nprojection_results.show(10)\nprojection_results.createOrReplaceTempView(\"projection_results\")"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nNilight_projection_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/65083402-f90f-4400-b673-d56dcbd53688/65083402-f90f-4400-b673-d56dcbd53688/65083402-f90f-4400-b673-d56dcbd53688/projection_IPW_final_launch/*\")\nNilight_projection_results \u003d Nilight_projection_results.withColumn(\"Brand\",lit(\"Nilight\"))\nOnePlus_projection_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/948b9355-445a-4930-b696-c1e336080e4b/948b9355-445a-4930-b696-c1e336080e4b/8dddae88-7be0-4899-8b3b-fd6c47615ce8/projection_IPW_final_launch/*\")\nOnePlus_projection_results \u003d OnePlus_projection_results.withColumn(\"Brand\",lit(\"OnePlus\"))\nColumbia_projection_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/eed00fe9-f36c-4e04-8396-2e1a523bfa27/eed00fe9-f36c-4e04-8396-2e1a523bfa27/7ed0ad45-fe64-42ec-99a8-702a6ba534dd/projection_IPW_final_launch/*\")\nColumbia_projection_results \u003d Columbia_projection_results.withColumn(\"Brand\",lit(\"Columbia\"))\nRainbocorns_projection_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/11c393ba-76c9-42f3-b07e-eaef14572c1f/11c393ba-76c9-42f3-b07e-eaef14572c1f/db59afb9-1007-4492-9c09-fbe9b126eca2/projection_IPW_final_launch/*\")\nRainbocorns_projection_results \u003d Rainbocorns_projection_results.withColumn(\"Brand\",lit(\"Rainbocorns\"))\nKingston_projection_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/762a62fe-f204-414e-8a75-66418488758a/762a62fe-f204-414e-8a75-66418488758a/0e583649-4277-4f5c-8eec-a966490441a8/projection_IPW_final_launch/*\")\nKingston_projection_results \u003d Kingston_projection_results.withColumn(\"Brand\",lit(\"Kingston\"))\nUtopia_Kitchen_projection_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/c0980a5a-867e-4c1f-a1cd-a2a3efaeff68/c0980a5a-867e-4c1f-a1cd-a2a3efaeff68/546d323b-944b-4b2f-9467-711f3d669540/projection_IPW_final_launch/*\")\nUtopia_Kitchen_projection_results \u003d Utopia_Kitchen_projection_results.withColumn(\"Brand\",lit(\"Utopia_Kitche\"))\nCatalonia_projection_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/39ca59c6-4467-4209-b848-1d820443750b/39ca59c6-4467-4209-b848-1d820443750b/2bc6aaa8-1af8-45af-826f-5cf0cb438ffc/projection_IPW_final_launch/*\")\nCatalonia_projection_results \u003d Catalonia_projection_results.withColumn(\"Brand\",lit(\"Catalonia\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfs \u003d [Nilight_projection_results, OnePlus_projection_results,Columbia_projection_results,Rainbocorns_projection_results,Kingston_projection_results, Utopia_Kitchen_projection_results,Catalonia_projection_results]\nprediction_union \u003d reduce(DataFrame.unionAll, dfs)\nprediction_union.select(\"Brand\").distinct().show()\nprediction_union.printSchema()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbrand \u003d \u0027Nilight\u0027\n# projection_results.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/sd_pilot_result/psm_model_projection/\"+brand+\"_psm_projection\")\nprediction_union.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/sd_pilot_result/psm_model_projection/psm_model_projection/aggregation/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select label, count(distinct customer_decoration_key) from feature_results group by 1 \").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select label, count(distinct customer_decoration_key) from projection_results group by 1 \").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select label, count(distinct customer_decoration_key) from modeldata_results group by 1 \").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select test_group_selected, count(distinct customer_decoration_key) from modeldata_results group by 1\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select label, min(rawPrediction), max(rawPrediction) from projection_results group by 1 \").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmodeldata_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/f056f7d5-2899-4afb-8d5a-ca42c86f2189/f056f7d5-2899-4afb-8d5a-ca42c86f2189/3e7999e3-09ea-4fe5-95f5-9bd120f75d11/model_data_IPW_final_launch/*\")\nmodeldata_results.printSchema()\nmodeldata_results.show(10)\nmodeldata_results.createOrReplaceTempView(\"modeldata_results\")"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n20230109_NBA\ns3://audience-generation-output-prod-na/82599fa8-7b41-4e8a-bc38-90d748db4093/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/model_data_IPW_final_launch/*\n20230109_H1_Brita\ns3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/model_data_IPW_final_launch/*\n20221208_Venmo_Caitlin\ns3://audience-generation-output-prod-na/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/48759272-2e28-46b4-bf27-72b44be43483/model_data_IPW_final_launch/*\n20221206_Ed_FX_Kindred_STV\ns3://audience-generation-output-prod-na/ead972d3-b50b-4626-b1b7-c48a85d2d728/ead972d3-b50b-4626-b1b7-c48a85d2d728/2f45a367-c6e9-4d2e-ae4c-1a77dc28b3b2/model_data_IPW_final_launch/*\n20221205_H1_Venmo\ns3://audience-generation-output-prod-na/2354f96c-4706-4a94-b455-3fb32a2d4c82/2354f96c-4706-4a94-b455-3fb32a2d4c82/5120ee9f-cf49-4a45-a0d5-9b245ce6bc11/model_data_IPW_final_launch/*\n20221109_Kodiak_Cakes\ns3://audience-generation-output-prod-na/f056f7d5-2899-4afb-8d5a-ca42c86f2189/f056f7d5-2899-4afb-8d5a-ca42c86f2189/3e7999e3-09ea-4fe5-95f5-9bd120f75d11/model_data_IPW_final_launch/*"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsurvey \u003d \u002720221109_Kodiak_Cakes\u0027\nmodeldata_results.repartition(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"s3u://fortunax/survey-analysis/adsp_bl_model/psm_model_input/\"+survey+\"_psm_model_input\")"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nModel_input1 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/48759272-2e28-46b4-bf27-72b44be43483/model_data_IPW_final_launch/*\")\nModel_input1 \u003d Model_input1.withColumn(\"Survey\",lit(\"20221208_Venmo_Caitlin\"))\nModel_input2 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/ead972d3-b50b-4626-b1b7-c48a85d2d728/ead972d3-b50b-4626-b1b7-c48a85d2d728/2f45a367-c6e9-4d2e-ae4c-1a77dc28b3b2/model_data_IPW_final_launch/*\")\nModel_input2 \u003d Model_input2.withColumn(\"Survey\",lit(\"20221206_Ed_FX_Kindred_STV\"))\nModel_input3 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/2354f96c-4706-4a94-b455-3fb32a2d4c82/2354f96c-4706-4a94-b455-3fb32a2d4c82/5120ee9f-cf49-4a45-a0d5-9b245ce6bc11/model_data_IPW_final_launch/*\")\nModel_input3 \u003d Model_input3.withColumn(\"Survey\",lit(\"20221205_H1_Venmo\"))\nModel_input4 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/f056f7d5-2899-4afb-8d5a-ca42c86f2189/f056f7d5-2899-4afb-8d5a-ca42c86f2189/3e7999e3-09ea-4fe5-95f5-9bd120f75d11/model_data_IPW_final_launch/*\")\nModel_input4 \u003d Model_input4.withColumn(\"Survey\",lit(\"20221109_Kodiak_Cakes\"))\n\ndfs \u003d [Model_input1,Model_input2,Model_input3,Model_input4]\nmodel_input_union \u003d reduce(DataFrame.unionAll, dfs)\nmodel_input_union.select(\"Survey\").distinct().show()\nmodel_input_union.printSchema()\n\nmodel_input_union.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/psm_model_input/aggregation\")\n\n# add new ADSP BL survey\n# model_input_union_old \u003d spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/psm_model_input/aggregation\")\n# Model_input5 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/model_data_IPW_final_launch/*\")\n# Model_input5 \u003d Model_input5.withColumn(\"Survey\",lit(\"20230109_H1_Brita\"))\n\n# dfs \u003d [model_input_union_old,Model_input5]\n# model_input_union \u003d reduce(DataFrame.unionAll, dfs)\n# model_input_union.select(\"Survey\").distinct().show()\n# model_input_union.printSchema()\n\n# model_input_union.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/psm_model_input/aggregation\")"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nControl1 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/48759272-2e28-46b4-bf27-72b44be43483/control_group_final_launch/*\")\nControl1 \u003d Control1.withColumn(\"Survey\",lit(\"20221208_Venmo_Caitlin\"))\nControl2 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/ead972d3-b50b-4626-b1b7-c48a85d2d728/ead972d3-b50b-4626-b1b7-c48a85d2d728/2f45a367-c6e9-4d2e-ae4c-1a77dc28b3b2/control_group_final_launch/*\")\nControl2 \u003d Control2.withColumn(\"Survey\",lit(\"20221206_Ed_FX_Kindred_STV\"))\nControl3 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/2354f96c-4706-4a94-b455-3fb32a2d4c82/2354f96c-4706-4a94-b455-3fb32a2d4c82/5120ee9f-cf49-4a45-a0d5-9b245ce6bc11/control_group_final_launch/*\")\nControl3 \u003d Control3.withColumn(\"Survey\",lit(\"20221205_H1_Venmo\"))\nControl4 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/f056f7d5-2899-4afb-8d5a-ca42c86f2189/f056f7d5-2899-4afb-8d5a-ca42c86f2189/3e7999e3-09ea-4fe5-95f5-9bd120f75d11/control_group_final_launch/*\")\nControl4 \u003d Control4.withColumn(\"Survey\",lit(\"20221109_Kodiak_Cakes\"))\n\ndfs \u003d [Control1,Control2,Control3,Control4]\ncontrol_union \u003d reduce(DataFrame.unionAll, dfs)\ncontrol_union.select(\"Survey\").distinct().show()\ncontrol_union.printSchema()\n\ncontrol_union.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/control_group_final_launch/aggregation\")\n\n\n# # add new ADSP BL survey\n# control_union_old \u003d  spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/control_group_final_launch/aggregation\")\n# Control5 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/control_group_final_launch/*\")\n# Control5 \u003d Control5.withColumn(\"Survey\",lit(\"20230109_H1_Brita\"))\n\n# dfs \u003d [control_union_old,Control5]\n# control_union \u003d reduce(DataFrame.unionAll, dfs)\n# control_union.select(\"Survey\").distinct().show()\n# control_union.printSchema()\n\n# control_union.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/control_group_final_launch/aggregation\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nTest1 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/48759272-2e28-46b4-bf27-72b44be43483/test_group_final_launch/*\")\nTest1 \u003d Test1.withColumn(\"Survey\",lit(\"20221208_Venmo_Caitlin\"))\nTest2 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/ead972d3-b50b-4626-b1b7-c48a85d2d728/ead972d3-b50b-4626-b1b7-c48a85d2d728/2f45a367-c6e9-4d2e-ae4c-1a77dc28b3b2/test_group_final_launch/*\")\nTest2 \u003d Test2.withColumn(\"Survey\",lit(\"20221206_Ed_FX_Kindred_STV\"))\nTest3 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/2354f96c-4706-4a94-b455-3fb32a2d4c82/2354f96c-4706-4a94-b455-3fb32a2d4c82/5120ee9f-cf49-4a45-a0d5-9b245ce6bc11/test_group_final_launch/*\")\nTest3 \u003d Test3.withColumn(\"Survey\",lit(\"20221205_H1_Venmo\"))\nTest4 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/f056f7d5-2899-4afb-8d5a-ca42c86f2189/f056f7d5-2899-4afb-8d5a-ca42c86f2189/3e7999e3-09ea-4fe5-95f5-9bd120f75d11/test_group_final_launch/*\")\nTest4 \u003d Test4.withColumn(\"Survey\",lit(\"20221109_Kodiak_Cakes\"))\n\ndfs \u003d [Test1,Test2,Test3,Test4]\ntest_union \u003d reduce(DataFrame.unionAll, dfs)\ntest_union.select(\"Survey\").distinct().show()\ntest_union.printSchema()\n\ntest_union.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/test_group_final_launch/aggregation\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprojection1 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/48759272-2e28-46b4-bf27-72b44be43483/projection_IPW_final_launch/*\")\nprojection1 \u003d projection1.withColumn(\"Survey\",lit(\"20221208_Venmo_Caitlin\"))\nprojection2 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/ead972d3-b50b-4626-b1b7-c48a85d2d728/ead972d3-b50b-4626-b1b7-c48a85d2d728/2f45a367-c6e9-4d2e-ae4c-1a77dc28b3b2/projection_IPW_final_launch/*\")\nprojection2 \u003d projection2.withColumn(\"Survey\",lit(\"20221206_Ed_FX_Kindred_STV\"))\nprojection3 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/2354f96c-4706-4a94-b455-3fb32a2d4c82/2354f96c-4706-4a94-b455-3fb32a2d4c82/5120ee9f-cf49-4a45-a0d5-9b245ce6bc11/projection_IPW_final_launch/*\")\nprojection3 \u003d projection3.withColumn(\"Survey\",lit(\"20221205_H1_Venmo\"))\nprojection4 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/f056f7d5-2899-4afb-8d5a-ca42c86f2189/f056f7d5-2899-4afb-8d5a-ca42c86f2189/3e7999e3-09ea-4fe5-95f5-9bd120f75d11/projection_IPW_final_launch/*\")\nprojection4 \u003d projection4.withColumn(\"Survey\",lit(\"20221109_Kodiak_Cakes\"))\n\ndfs \u003d [projection1, projection2, projection3, projection4]\nprojection_union \u003d reduce(DataFrame.unionAll, dfs)\nprojection_union.select(\"Survey\").distinct().show()\nprojection_union.printSchema()\n\nprojection_union.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/projection/aggregation\")"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmodel_input_union \u003d spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/psm_model_input/aggregation/*\")\ncontrol_union \u003dspark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/control_group_final_launch/aggregation/*\")\ntest_union \u003d spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/test_group_final_launch/aggregation/*\")\nprojection_union \u003d spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/projection/aggregation/*\")"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# join model input data to psm model selected group\ndfs \u003d [control_union.select(\"customer_decoration_key\",\"Survey\").withColumn(\"Type\",lit(\"Control\")),test_union.select(\"customer_decoration_key\",\"Survey\").withColumn(\"Type\",lit(\"Test\"))]\ngroup_union \u003d reduce(DataFrame.unionAll, dfs)\n\nfinal_agg \u003d model_input_union.join(group_union, [\"customer_decoration_key\",\"Survey\"],\"left\")\\\n                             .join(projection_union.select(\"customer_decoration_key\",\"Survey\",\"rawPrediction\"), [\"customer_decoration_key\",\"Survey\"],\"left\")\n\nfinal_agg.printSchema()           \n\nfinal_agg.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/final_agg/\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Add new ADSP BL survey\nModel_input5 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/model_data_IPW_final_launch/*\")\nModel_input5 \u003d Model_input5.withColumn(\"Survey\",lit(\"20230109_H1_Brita\"))\nControl5 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/control_group_final_launch/*\")\nControl5 \u003d Control5.withColumn(\"Survey\",lit(\"20230109_H1_Brita\"))\nTest5 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/test_group_final_launch/*\")\nTest5 \u003d Test5.withColumn(\"Survey\",lit(\"20230109_H1_Brita\"))\nprojection5 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/projection_IPW_final_launch/*\")\nprojection5 \u003d projection5.withColumn(\"Survey\",lit(\"20230109_H1_Brita\"))\n\nfinal_agg \u003d spark.read.parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/final_agg/\")\nfinal_agg.printSchema()\n\ndfs \u003d [Control5.select(\"customer_decoration_key\",\"Survey\").withColumn(\"Type\",lit(\"Control\")),Test5.select(\"customer_decoration_key\",\"Survey\").withColumn(\"Type\",lit(\"Test\"))]\ngroup_union \u003d reduce(DataFrame.unionAll, dfs)\n\nfinal_agg5 \u003d Model_input5.join(group_union, [\"customer_decoration_key\",\"Survey\"],\"left\")\\\n                         .join(projection5.select(\"customer_decoration_key\",\"Survey\",\"rawPrediction\"), [\"customer_decoration_key\",\"Survey\"],\"left\")\nfinal_agg5.printSchema()    \n\ndfs2 \u003d [final_agg, final_agg5]\nfinal_agg_new \u003d reduce(DataFrame.unionAll, dfs2)\nfinal_agg_new.printSchema()    \n\nfinal_agg_new.select(\"Survey\").distinct().show()\n\nfinal_agg_new.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/final_agg_new/\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsqlContext.setConf(\"spark.driver.maxResultSize\",\"30g\")\nsqlContext.setConf(\"spark.sql.shuffle.partitions\", \"10\")\nsqlContext.setConf(\"spark.sql.adaptive.enabled\", \"true\")\nsqlContext.setConf(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n\n# survey_list \u003d spark.read.csv(\"s3://fortunax/survey-analysis/adsp_bl_model/100surveylist/ADSP_SurveyAll.csv\", header \u003d True)\nsurvey_list  \u003dspark.read.csv(\"s3://fortunax/survey-analysis/adsp_bl_model/100surveylist/ADSP_SurveyAll0127.csv\", header \u003d True)\\\n                        .withColumn(\"Created_Date\",date_format(\"Created_Date\",\"YYYY-MM-dd\").cast(\"date\"))\\\n                        .withColumnRenamed(\"Survey_Title\",\"Survey\")\\\n                         .withColumnRenamed(\"StudyID\",\"SurveyId\")\nsurvey_list.printSchema()\nname_dict \u003d {row[\u0027SurveyId\u0027]: row[\u0027Survey\u0027] for row in survey_list.collect()}\nprint(name_dict)\n\n\nschema \u003d spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/final_agg_new/*\").withColumn(\"SurveyId\", lit(\"NA\")).schema\n\n# schema1 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/model_data_IPW_final_launch/*\")\\\n#               .withColumn(\"Survey\",lit(\"NA\")).schema\n# schema2 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/control_group_final_launch/*\")\\\n#               .withColumn(\"Survey\",lit(\"NA\")).schema\n# schema3 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/test_group_final_launch/*\")\\\n#              .withColumn(\"Survey\",lit(\"NA\")).schema\n# schema4 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/projection_IPW_final_launch/*\")\\\n#               .withColumn(\"Survey\",lit(\"NA\")).schema\n\nfinal_agg100 \u003d spark.createDataFrame([], schema)\nno_result \u003d 0\n\nfor surveyid, surveyname in name_dict.items(): \n    try: \n        # model input\n        print(surveyid, surveyname)\n        indi_survey \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/model_data_IPW_final_launch/\")\n        indi_survey \u003d  indi_survey.withColumn(\"Survey\",lit(surveyname))\n        # control and test\n        indi_control \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/control_group_final_launch/*\")\\\n                                 .withColumn(\"Survey\",lit(surveyname))\\\n                                 .withColumn(\"Type\",lit(\"Control\"))\\\n                                 .select(\"customer_decoration_key\",\"Survey\",\"Type\")\n        indi_test \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/test_group_final_launch/*\")\\\n                                 .withColumn(\"Survey\",lit(surveyname))\\\n                                 .withColumn(\"Type\",lit(\"Test\"))\\\n                                 .select(\"customer_decoration_key\",\"Survey\",\"Type\")\n                                 \n        dfs_tc \u003d [indi_control, indi_test]\n        tc_union \u003d reduce(DataFrame.unionAll, dfs_tc)\n        # projection\n        indi_project \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/projection_IPW_final_launch/*\")\\\n                                 .withColumn(\"Survey\",lit(surveyname))\\\n                                 .select(\"customer_decoration_key\",\"Survey\",\"rawPrediction\")\n        # final agg individual survey\n        indi_final_agg \u003d indi_survey.join(tc_union, [\"customer_decoration_key\",\"Survey\"],\"left\")\\\n                                    .join(indi_project, [\"customer_decoration_key\",\"Survey\"],\"left\")\\\n                                    .withColumn(\"SurveyId\", lit(surveyid))\n                                    \n        indi_final_agg.groupBy(\"Survey\").agg(F.count(\"test_group_selected\")).show(truncate \u003d False)\n        # union all survey\n        dfs \u003d [final_agg100,  indi_final_agg]\n        final_agg100 \u003d reduce(DataFrame.unionAll, dfs).distinct()\n        \n        continue\n    \n    except: \n        print(f\"{surveyid}, {surveyname}  does not have model generated results\")\n        no_result +\u003d 1\n        \nfinal_agg100.groupBy([\"Survey\"]).agg(F.count(\"test_group_selected\")).show(200, truncate \u003d False)\nfinal_agg100.agg(F.countDistinct(\"Survey\")).show()\nprint(f\"Total {no_result} surveys no found model generated results\")\n\n# join survey info\nif \"Created_Date\" not in final_agg100.schema.fieldNames():\n    final_agg100 \u003d final_agg100.join(survey_list, \"SurveyId\",\"left\").drop(survey_list.SurveyId).drop(survey_list.Survey)\n\n# add old adsp survey\nfinal_agg_add \u003d  spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/100final_agg/*\").filter(\"Survey\u003d\u0027Starbucks_AudSurvey_SnS_Dec22\u0027\")\\\n                      .join(survey_list, \"Survey\",\"left\").drop(survey_list.Survey)\noffline_bl \u003d spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/final_agg_new/*\")\\\n                      .join(survey_list, \"Survey\",\"left\").drop(survey_list.Survey)\nfinal_agg100_new \u003d final_agg100.unionByName(final_agg_add).unionByName(offline_bl)\nfinal_agg100_new.agg(F.countDistinct(\"Survey\")).show()\n\n# write to s3\nfinal_agg100_new.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/100final_agg_0128/\")\n        "
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nsqlContext.setConf(\"spark.driver.maxResultSize\",\"30g\")\nsqlContext.setConf(\"spark.sql.shuffle.partitions\", \"10\")\nsqlContext.setConf(\"spark.sql.adaptive.enabled\", \"true\")\nsqlContext.setConf(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n\n# survey_list \u003d spark.read.csv(\"s3://fortunax/survey-analysis/adsp_bl_model/100surveylist/ADSP_SurveyAll.csv\", header \u003d True)\nsurvey_list  \u003dspark.read.csv(\"s3://fortunax/survey-analysis/adsp_bl_model/100surveylist/ADSP_Survey_All_0228.csv\", header \u003d True)\\\n                        .withColumn(\"Created_Date\",date_format(\"Created_Date\",\"YYYY-MM-dd\").cast(\"date\"))\\\n                        .withColumnRenamed(\"Survey_Title\",\"Survey\")\\\n                         .withColumnRenamed(\"StudyID\",\"SurveyId\")\nsurvey_list.printSchema()\nname_dict \u003d {row[\u0027SurveyId\u0027]: row[\u0027Survey\u0027] for row in survey_list.collect()}\nprint(name_dict)\n\n\nschema \u003d spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/final_agg_new/*\").withColumn(\"SurveyId\", lit(\"NA\")).schema\n\n# schema1 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/model_data_IPW_final_launch/*\")\\\n#               .withColumn(\"Survey\",lit(\"NA\")).schema\n# schema2 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/control_group_final_launch/*\")\\\n#               .withColumn(\"Survey\",lit(\"NA\")).schema\n# schema3 \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/test_group_final_launch/*\")\\\n#              .withColumn(\"Survey\",lit(\"NA\")).schema\n# schema4 \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/projection_IPW_final_launch/*\")\\\n#               .withColumn(\"Survey\",lit(\"NA\")).schema\n\nfinal_agg100 \u003d spark.createDataFrame([], schema)\nno_result \u003d 0\n\nfor surveyid, surveyname in name_dict.items(): \n    try: \n        # model input\n        print(surveyid, surveyname)\n        indi_survey \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/model_data_IPW_final_launch/\")\n        indi_survey \u003d  indi_survey.withColumn(\"Survey\",lit(surveyname))\n        # control and test\n        indi_control \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/control_group_final_launch/*\")\\\n                                 .withColumn(\"Survey\",lit(surveyname))\\\n                                 .withColumn(\"Type\",lit(\"Control\"))\\\n                                 .select(\"customer_decoration_key\",\"Survey\",\"Type\")\n        indi_test \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/test_group_final_launch/*\")\\\n                                 .withColumn(\"Survey\",lit(surveyname))\\\n                                 .withColumn(\"Type\",lit(\"Test\"))\\\n                                 .select(\"customer_decoration_key\",\"Survey\",\"Type\")\n                                 \n        dfs_tc \u003d [indi_control, indi_test]\n        tc_union \u003d reduce(DataFrame.unionAll, dfs_tc)\n        # projection\n        indi_project \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/projection_IPW_final_launch/*\")\\\n                                 .withColumn(\"Survey\",lit(surveyname))\\\n                                 .select(\"customer_decoration_key\",\"Survey\",\"rawPrediction\")\n        # final agg individual survey\n        indi_final_agg \u003d indi_survey.join(tc_union, [\"customer_decoration_key\",\"Survey\"],\"left\")\\\n                                    .join(indi_project, [\"customer_decoration_key\",\"Survey\"],\"left\")\\\n                                    .withColumn(\"SurveyId\", lit(surveyid))\n                                    \n        indi_final_agg.groupBy(\"Survey\").agg(F.count(\"test_group_selected\")).show(truncate \u003d False)\n        # union all survey\n        dfs \u003d [final_agg100,  indi_final_agg]\n        final_agg100 \u003d reduce(DataFrame.unionAll, dfs).distinct()\n        \n        continue\n    \n    except: \n        print(f\"{surveyid}, {surveyname}  does not have model generated results\")\n        no_result +\u003d 1\n        \nfinal_agg100.groupBy([\"Survey\"]).agg(F.count(\"test_group_selected\")).show(200, truncate \u003d False)\nfinal_agg100.agg(F.countDistinct(\"Survey\")).show()\nprint(f\"Total {no_result} surveys no found model generated results\")\n\n# join survey info\nif \"Created_Date\" not in final_agg100.schema.fieldNames():\n    final_agg100 \u003d final_agg100.join(survey_list, \"SurveyId\",\"left\").drop(survey_list.SurveyId).drop(survey_list.Survey)\n\n\n# write to s3\nfinal_agg100.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/100final_agg_0228/\")\n        "
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Remove duplicated columns - need to run multiple times if more than 2 duplicates\nif \"Created_Date\" not in final_agg100_s.schema.fieldNames()):\n    final_agg100_s \u003d final_agg100.join(survey_list, \"SurveyId\",\"left\").drop(survey_list.SurveyId)\nfinal_agg100_s.printSchema()\n# drop duplicated columns\ndf_cols \u003d final_agg100_s.columns\n# get index of the duplicate columns\nduplicate_col_index \u003d list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) \u003e\u003d 2]))\n\n# rename by adding suffix \u0027_duplicated\u0027\nfor i in duplicate_col_index:\n    df_cols[i] \u003d df_cols[i] + \u0027_duplicated\u0027\n\n# rename the column in DF\nfinal_agg100_s \u003d final_agg100_s.toDF(*df_cols)\n\n# remove flagged columns\ncols_to_remove \u003d [c for c in df_cols if \u0027_duplicated\u0027 in c]\nfinal_agg100_s \u003d final_agg100_s.drop(*cols_to_remove)\nfinal_agg100_s.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_agg100_new \u003d final_agg100_s.unionByName(final_agg_add).unionByName(offline_bl)\nfinal_agg100_new.agg(F.countDistinct(\"Survey\")).show()\n\n# write to s3\nfinal_agg100_new.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/100final_agg_0128/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_agg100_check \u003d  spark.read.parquet(\"s3://fortunax/survey-analysis/adsp_bl_model/100final_agg_0228/*\")\nfinal_agg100_check.printSchema()\nfinal_agg100_check.agg(F.countDistinct(\"SurveyId\")).show(300, truncate \u003d False)\nfinal_agg100_check.select(\"Survey\",\"SurveyId\").distinct().orderBy(\"Survey\").show(300, truncate \u003d False)\nfinal_agg100_check.groupBy(\"Survey\",\"SurveyId\").agg(F.count(\"Type\")).show(300, truncate \u003d False)\nfinal_agg100_check.groupBy(\"segment\").agg(F.countDistinct(\"customer_decoration_key\")).show(300, truncate \u003d False)\nfinal_agg100_check.select(\"purchase_segment_code\",\"recency_segment_id\",\"tenure_segment_id\",\"segment\").distinct().show(300, truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nshadow_test \u003d [\u00271010199001735609393\u0027,\n \u00271013478236810494046\u0027,\n \u00271011410602925589885\u0027,\n \u00271022953695740577527\u0027,\n \u00271016559059479652690\u0027,\n \u00271021307103947166529\u0027,\n \u00271024260143819101763\u0027,\n \u00271025409659045532045\u0027,\n \u00271019913940017188116\u0027,\n \u00271022500192045985969\u0027,\n \u00271015215940946229133\u0027,\n \u00271018995918905193997\u0027,\n \u00271010391264163697834\u0027]\n# final_agg100_check_shadow \u003d final_agg100_check.filter(F.col(\"SurveyId\").isin(shadow_test))\nfinal_agg100_check_shadow.agg(F.countDistinct(\"SurveyId\")).show(300, truncate \u003d False)\n# final_agg100_check_shadow.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/100final_agg_0314_shadow2/\")\n# final_agg100_check.repartition(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"s3u://fortunax/survey-analysis/100final_agg_0314_shadow2/\")\n\nfor survey_id in shadow_test:\n    df_test \u003dfinal_agg100_check_shadow.filter((F.col(\"SurveyId\")\u003d\u003dsurvey_id) \u0026 (F.col(\"Type\")\u003d\u003d\"Test\"))\n    df_control \u003dfinal_agg100_check_shadow.filter((F.col(\"SurveyId\")\u003d\u003dsurvey_id) \u0026 (F.col(\"Type\")\u003d\u003d\"Control\"))\n    df_test.repartition(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"s3u://fortunax/survey-analysis/brand_lift_test\u0026control/\"+survey_id+\"/test/\")\n    df_control.repartition(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"s3u://fortunax/survey-analysis/brand_lift_test\u0026control/\"+survey_id+\"/control/\")\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsurveyid \u003d \u00271016799845470573444\u0027\n# indi_check \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/test_group_final_launch/*\")\nindi_check \u003d  spark.read.parquet(\"s3://audience-generation-output-prod-na/\"+surveyid+\"/*/*/model_data_IPW_final_launch/\")\nindi_check.agg(F.countDistinct(\"customer_decoration_key\"),F.count(\"customer_decoration_key\")).show(300, truncate \u003d False)\nindi_check.agg(F.count(\"test_group_selected\")).show(300, truncate \u003d False)\nindi_check.select(\"customer_decoration_key\",\"test_group_selected\").distinct().agg(F.count(\"test_group_selected\")).show(300, truncate \u003d False)\nfinal_agg100_check.filter(F.col(\"SurveyId\") \u003d\u003d surveyid).distinct().groupBy(\"Survey\",\"SurveyId\").agg(F.count(\"Type\")).show(300, truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsurvey_list  \u003dspark.read.csv(\"s3://fortunax/survey-analysis/adsp_bl_model/100surveylist/ADSP_Survey_All_0228.csv\", header \u003d True)\\\n                        .withColumn(\"Created_Date\",date_format(\"Created_Date\",\"YYYY-MM-dd\").cast(\"date\"))\nbias_diff \u003d spark.read.csv(\"s3://klensink-dev/surveys/shadow-test-control-groups/2023-03-03/bias_improvement.csv\", header \u003d True)\nsurvey_bias_join \u003d survey_list.join(bias_diff.select(\"diff\",\"SurveyId\"),survey_list.StudyId \u003d\u003d bias_diff.SurveyId, \"left\").drop(bias_diff.SurveyId)\\\n                              .withColumn(\"StudyId\",col(\"StudyId\").cast(\"string\"))\nsurvey_bias_join.printSchema()\nsurvey_bias_join.show()\nsurvey_bias_join.repartition(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"s3u://fortunax/survey-analysis/adsp_bl_model/100surveylist/ADSP_Survey_All_0228_with_bias\")"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# number of survey per panelist\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).agg(F.max(\"Survey\")).show()\nmax_survey \u003d final_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).agg(F.max(\"Survey\")).collect()[0][0]\nprint(max_survey)\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).orderBy(F.col(\"Survey\").desc()).show(truncate \u003d False)\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).groupBy(\"Survey\").agg(F.countDistinct(\"customer_decoration_key\").alias(\"Count_Cust\")).orderBy(\"Count_Cust\").show(truncate \u003d False)\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).agg(F.countDistinct(\"customer_decoration_key\")).show()\n\n### by type\nfinal_agg100_check.filter(F.col(\"Type\")\u003d\u003d\u0027Test\u0027).groupBy(\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).groupBy(\"Survey\").agg(F.countDistinct(\"customer_decoration_key\").alias(\"Count_Cust\")).orderBy(\"Count_Cust\").show(truncate \u003d False)\nfinal_agg100_check.filter(F.col(\"Type\")\u003d\u003d\u0027Control\u0027).groupBy(\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).groupBy(\"Survey\").agg(F.countDistinct(\"customer_decoration_key\").alias(\"Count_Cust\")).orderBy(\"Count_Cust\").show(truncate \u003d False)\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"Type\").agg(F.countDistinct(\"customer_decoration_key\")).show()\n\n# final_agg100_check.filter(F.col(\"Type\").isNotNull()).select(\"customer_decoration_key\",\"Survey\").filter(F.col(\"customer_decoration_key\") \u003d\u003d \u002701010e41e1dde97d5696d2ce3bdd4d4a14df64b2673c17fd65ba65b7a98da4825613\u0027).distinct().show(truncate \u003d False)\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"Survey\",\"Type\").agg(F.countDistinct(\"customer_decoration_key\")).show(truncate \u003d False)\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"Survey\",\"Type\").agg(F.countDistinct(\"customer_decoration_key\").alias(\"Count_Cust\")).agg(F.mean(\"Count_Cust\")).show(truncate \u003d False)\n\n### by create day\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"Created_Date\",\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).orderBy(F.col(\"Survey\").desc()).show(truncate \u003d False)\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"Created_Date\",\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).groupBy(\"Survey\").agg(F.countDistinct(\"customer_decoration_key\").alias(\"Count_Cust\")).orderBy(\"Count_Cust\").show(truncate \u003d False)\n\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).filter(F.col(\"customer_decoration_key\") \u003d\u003d \u00270101e130ef7ab0d6dcf468f39ffc3668c5dc6d3722e477c0021e09a580ca79b91063\u0027).filter(F.col(\"Created_Date\") \u003d\u003d \u00272022-12-30\u0027).select(\"Survey\",\"Type\").distinct().show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_agg100_check.filter(F.col(\"Type\").isNotNull()).agg(F.countDistinct(\"customer_decoration_key\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nz.show(final_agg100_check.filter(F.col(\"Type\")\u003d\u003d\u0027Control\u0027).groupBy(\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).groupBy(\"Survey\").agg(F.countDistinct(\"customer_decoration_key\").alias(\"Count_Cust\")).orderBy(\"Count_Cust\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# by day\nz.show(final_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"Created_Date\",\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).groupBy(\"Survey\").agg(F.countDistinct(\"customer_decoration_key\").alias(\"Count_Cust\")).orderBy(\"Count_Cust\"))\nz.show(final_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"Created_Date\",\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).groupBy(\"Created_Date\").agg(F.max(\"Survey\").alias(\"Max_Survey\")).orderBy(\"Created_Date\"))\nz.show(final_agg100_check.filter(F.col(\"Type\").isNotNull()).groupBy(\"Created_Date\",\"customer_decoration_key\").agg(F.countDistinct(\"SurveyId\").alias(\"Survey\")).select(\"Created_Date\",\"Survey\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nshadow_test_list \u003d [\u00271015930497617062914\u0027,\u00271012205384885535143\u0027,\u00271022024940649855646\u0027,\u00271018740571717925961\u0027,\u00271015352820333445760\u0027,\u00271013328087937099804\u0027,\u00271009907040961550818\u0027,\u00271024809370913572286\u0027,\u00271015750600094221428\u0027,\u00271015381936683407494\u0027,\u00271009186255172395887\u0027,\u00271016157025496975007\u0027,\u00271022446369444034105\u0027,\u00271012657064956340970\u0027,\u00271015348741617230198\u0027]\nfinal_agg100_check.filter(F.col(\"SurveyId\").isin(shadow_test_list)).filter(F.col(\"Type\").isNotNull()).groupBy(\"Survey\",\"Type\").agg(F.countDistinct(\"customer_decoration_key\")).orderBy(\"Survey\",\"Type\").show(100, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_agg100.groupBy([\"Survey\"]).agg(F.count(\"test_group_selected\")).alias(\"test\").agg(F.concat_ws(\"|\",F.collect_list(F.col(\"Survey\")))).first()[0]\n# final_agg100.select(\"Survey\").distinct().agg(F.concat_ws(\"|\",F.collect_list(F.col(\"Survey\")))).first()[0]"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import monotonically_increasing_id \nfrom pyspark.sql.window import Window  \nfinal_agg_100check \u003d  spark.read.parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/100final_agg/\")\nw \u003d Window.orderBy(\"Survey\")\n# final_agg100.select(\"Survey\").distinct().withColumn(\"id\", monotonically_increasing_id()).show(100)\nfinal_agg_100check.select(\"Survey\").distinct().withColumn(\"id\", row_number().over(w)).select(\"id\",\"Survey\").show(300, truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_agg_100check \u003d  spark.read.parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/100final_agg/\")\nmodel_input100check \u003d  spark.read.parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/100psm_model_input/\")\nmodel_input100check.select(\"Survey\").distinct().join(final_agg_100check.select(\"Survey\").distinct(), \"Survey\", \"leftanti\").show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_agg_new \u003d spark.read.parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/final_agg_new/\")\nfinal_agg_new.printSchema()\n\nfinal_agg_new.filter(F.col(\"Type\") \u003d\u003d \u0027Test\u0027).groupBy(\"Survey\").agg(F.count(\"test_group_selected\"), F.count(\"Type\"),F.count(\"rawPrediction\")).show()\nfinal_agg_new.filter(F.col(\"Type\") \u003d\u003d \u0027Control\u0027).groupBy(\"Survey\").agg(F.count(\"test_group_selected\"), F.count(\"Type\"),F.count(\"rawPrediction\")).show()\nfinal_agg_new.groupBy([\"Survey\",\"Type\"]).agg(F.count(\"test_group_selected\"), F.count(\"Type\")).orderBy(\"Survey\",\"Type\").show()\nfinal_agg_new.groupBy([\"Survey\",\"test_group_selected\"]).agg(F.count(\"test_group_selected\"), F.count(\"Type\")).orderBy(\"Survey\",\"test_group_selected\").show()\nfinal_agg_new.filter((F.col(\"test_group_selected\")\u003d\u003d\u0027Y\u0027) \u0026 (F.col(\"Type\") !\u003d \u0027Test\u0027)).show()\nfinal_agg_new.filter(F.col(\"test_group_selected\").isNull()).filter(F.col(\"Type\").isNotNull()).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_agg_all \u003d spark.read.option(\"header\", True).csv(\"s3://science-onlinepanel/klensink-dev/surveys/abl_sd_adsp_agg.csv\")\nfinal_agg_all.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfinal_agg_response \u003d  spark.read.parquet(\"s3u://qqian-test/temp_data/feature_selection/\")\nfinal_agg_response.printSchema()\nfinal_agg_response.filter(F.col(\"has_qualified_answer\").isNotNull()).groupby(\"Survey\").agg(F.countDistinct(\"customer_decoration_key\")).show()\nfinal_agg_response.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/feature_selection/\")\n\nfinal_agg_response_bl \u003d final_agg_response.filter(F.col(\"has_qualified_answer\").isNotNull()).filter(\"question_sequence_number \u003d\u003d \u00271\u0027\")\nfinal_agg_response_bl.groupby(\"Survey\").agg(F.countDistinct(\"customer_decoration_key\")).show()\n\nfinal_agg_response_bl.repartition(1).write.mode(\"overwrite\").option(\"header\", True).parquet(\"s3u://fortunax/survey-analysis/adsp_bl_model/feature_selection_bl/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeature_results \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/44d145d6-8a55-4bd5-9e50-a0713e261218/44d145d6-8a55-4bd5-9e50-a0713e261218/3b7e8b8d-1422-4400-aaa7-0fb3f0048884/featurized_data_IPW_final_launch/*\")\nfeature_results.printSchema()\nfeature_results.show(10, truncate \u003d False)\nfeature_results.createOrReplaceTempView(\"feature_results\")"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprojection1.agg(F.count(\"customer_decoration_key\"), F.countDistinct(\"customer_decoration_key\")).show()\nTest1.agg(F.count(\"customer_decoration_key\"), F.countDistinct(\"customer_decoration_key\")).show()\nControl1.agg(F.count(\"customer_decoration_key\"), F.countDistinct(\"customer_decoration_key\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nControl1s \u003d spark.read.option(\"header\", True).csv(\"s3://audience-generation-output-prod-na/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/48759272-2e28-46b4-bf27-72b44be43483/control_sample_final_launch/*\")\nControl1s.printSchema()\nControl1s.agg(F.count(\"customer_decoration_key\"), F.countDistinct(\"customer_decoration_key\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nTest1s \u003d spark.read.parquet(\"s3://audience-generation-output-prod-na/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/c9fbc167-edf3-4c39-9748-b1edc0cdaf92/48759272-2e28-46b4-bf27-72b44be43483/test_sample_final_launch/*\")\nTest1s.printSchema()\nTest1s.agg(F.count(\"customer_decoration_key\"), F.countDistinct(\"customer_decoration_key\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n## dim_campaign\ndim_campaign \u003d spark.sql(\"\"\"\nselect *\nfrom spektr_campaign.dim_campaigns\nwhere marketplace_id \u003d 1\n\"\"\")\ndim_campaign.printSchema()\ndim_campaign.show()\n\ndim_campaign.filter(F.col(\"campaign_id\").isin([579861703287912924,593949548380621494,580036844078265991,590778467115612870,588546538790250873])).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndim_campaign.select(\"program_type\").distinct().show(truncate \u003d False)\n\ndim_campaign.select(\"experiment_id\").distinct().show(5, truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// onehotencoder\nimport org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder,OneHotEncoderEstimator, VectorAssembler }\nimport org.apache.spark.ml.Pipeline\n\nval df \u003d sqlContext.createDataFrame(Seq(\n  (0, \"a\",\"d\"),\n  (1, \"b\",\"e\"),\n  (2, \"c\",\"c\"),\n  (3, null,\"c\"),         //\u003c- original example has \"a\" here\n  (4, \"a\",null),\n  (5, \"c\",\"d\")\n)).toDF(\"id\", \"category1\",\"category2\")\n\nval dummy_col_set \u003d Array(\"categoryIndex1\", \"categoryIndex2\")\n\nval pre_dummy \u003d dummy_col_set\nval post_dummy \u003d pre_dummy.map(_ + \"_vector\")\nval assembleCols \u003d post_dummy\n    \nval indexer1 \u003d new StringIndexer()\n  .setHandleInvalid(\"keep\") //default is \"error\", or \"skip\", \"keep\"\n  .setInputCol(\"category1\")\n  .setOutputCol(\"categoryIndex1\")\n\nval indexer2 \u003d new StringIndexer()\n  .setHandleInvalid(\"keep\") //default is \"error\", or \"skip\", \"keep\"\n  .setInputCol(\"category2\")\n  .setOutputCol(\"categoryIndex2\")\n\n// val indexed \u003d indexer1.transform(df)\n\n// val encoder \u003d new OneHotEncoder()\n//   .setInputCol(\"categoryIndex1\")\n//   .setOutputCol(\"categoryIndex1Encoded\")\n  \nval encoder \u003d new OneHotEncoderEstimator()\n  .setInputCols(pre_dummy)\n  .setOutputCols(post_dummy)\n\n// val encoded \u003d encoder.transform(indexed)\n\n// encoded.show()\n\nval feature_assembler \u003d new VectorAssembler()\n      .setInputCols(assembleCols)\n      .setOutputCol(\"featureVector\")\n\nval feature_pipe \u003d new Pipeline()\n      .setStages(Array(indexer1, indexer2, encoder, feature_assembler))\n\nval FeatureModel \u003d feature_pipe.fit(df)\n\nval FeatureModel2 \u003d feature_pipe.fit(df.na.fill(\"0\"))\n\nval featurized_data \u003d   FeatureModel.transform(df)\nfeaturized_data.show()\n\nval featurized_data_na \u003d   FeatureModel.transform(df).na.fill(\"0\")  \nfeaturized_data_na.show()\n\nval featurized_data_na2 \u003d   FeatureModel2.transform(df)\nfeaturized_data_na2.show()\n\nval featurized_data_na3 \u003d   FeatureModel2.transform(df).na.fill(\"0\")  \nfeaturized_data_na3.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# taxonomy\ntaxonomy \u003d spark.read.parquet(\"s3://panel-cradle-testing/2023-02-01/*\")\ntaxonomy.printSchema()\ntaxonomy.show(100,  False)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntaxonomy.select(\"segmentId\",\"taxonomy.localizedCategories.en.path.name\",\"taxonomy.localizedCategories.en.path.id\",\"taxonomy.description\",\"taxonomy.name\",\"taxonomy.id\").show(truncate \u003d False)\ntaxonomy.agg(F.count(\"segmentId\")).show()\ntaxonomy.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in [\"taxonomy.description\",\"taxonomy.name\",\"taxonomy.id\",\"segmentId\"]]).show(truncate \u003d False)\ntaxonomy.select(F.col(\"segmentId\").cast(\"Integer\")).agg(F.max(\"segmentId\")).show(1000,truncate \u003d False)\ntaxonomy.select(\"taxonomy.localizedCategories.en.path.name\").distinct().show(300,truncate \u003d False)\n# taxonomy_new \u003d taxonomy.withColumn(\"cate_name\",\"taxonomy.localizedCategories.en.path.name\")\n# split_col \u003d pyspark.sql.functions.split(taxonomy_new[\"cate_name\"], \u0027, \u0027)\n# taxonomy_new.select(split_col.getItem(1)).alias(\"name\").distinct().show(300,truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntaxonomy.select(F.col(\"segmentId\").cast(\"Integer\")).agg(F.max(\"segmentId\")).show(1000,truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntaxonomy.select(\"taxonomy.localizedCategories.en.path.name\",\"taxonomy.localizedCategories.en.path.id\").filter(\"\u0027taxonomy.localizedCategories.en.path.id\u0027 in (17,18,19)\").distinct().show(1000,truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nin_market_seg \u003d spark.read.parquet(\"s3://customers-segments-data-prod-na/SegmentsData/PanelCustomersSegmentsData/denormalizedData/2023/02/01/*\")\nin_market_seg.show()\nin_market_seg2 \u003d spark.read.parquet(\"s3://customers-segments-data-prod-na/SegmentsData/PanelCustomersSegmentsData/encodedData/2023/02/01/*\")\nin_market_seg2.show()\nin_market_seg3 \u003d spark.read.parquet(\"s3://customers-segments-data-prod-na/SegmentsData/PanelCustomersSegmentsData/Bucketed/2023/02/01/*\")\nin_market_seg3.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nraking_check \u003d spark.read.parquet(\"s3://analytics-emr-output-bucket/raking_bucket/10/\")\nraking_check.show(5, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfeature_selection \u003d spark.read.parquet(\"s3://qqian-test/temp_data/feature_selection/\")\nfeature_selection.show(2, False)\nfeature_selection.groupBy(\"Survey\").agg(F.sum(\"has_qualified_answer\")).show(300, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"select distinct transaction_date from spektr_shopperpanel.panel_transactions_v3\"\"\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}