{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5NDXrN2CtH7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (1.16.4)\n",
      "Requirement already satisfied: pandas in /anaconda3/lib/python3.7/site-packages (0.25.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /anaconda3/lib/python3.7/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /anaconda3/lib/python3.7/site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /anaconda3/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.7/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: sklearn in /anaconda3/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.7/site-packages (from sklearn) (0.21.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: pyLDAvis in /anaconda3/lib/python3.7/site-packages (2.1.2)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.13.2)\n",
      "Requirement already satisfied: numexpr in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.6.9)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.17.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.25.3)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.10.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.16.4)\n",
      "Requirement already satisfied: funcy in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.14)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.33.4)\n",
      "Requirement already satisfied: pytest in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (5.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /anaconda3/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.8.0)\n",
      "Requirement already satisfied: packaging in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (7.0.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.17)\n",
      "Requirement already satisfied: wcwidth in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.1.7)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda3/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (2.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->pyLDAvis) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "#the module 'sys' allows istalling module from inside Jupyter\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Natrual Language ToolKit (NLTK)\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "\n",
    "!{sys.executable} -m pip install sklearn\n",
    "from sklearn import metrics\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import  CountVectorizer #bag-of-words vectorizer \n",
    "from sklearn.decomposition import LatentDirichletAllocation #package for LDA\n",
    "\n",
    "# Plotting tools\n",
    "\n",
    "from pprint import pprint\n",
    "!{sys.executable} -m pip install pyLDAvis #visualizing LDA\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#ignore warnings about future changes in functions as they take too much space\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "def get_topic_words(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_words = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_word_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_words.append(keywords.take(top_word_locs).tolist())\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "7igYuRaV4bF7",
    "outputId": "c0595e40-c850-43b2-d32a-43cb2cbd0e1f"
   },
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "df_fashion = pd.read_json('data/AMAZON_FASHION_5.json.gz', lines = True, compression='gzip')\n",
    "df_beauty = pd.read_json('data/All_Beauty_5.json.gz', lines = True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "knpHP22w4scK",
    "outputId": "b75be678-1377-4b8c-b689-56e1eab703b8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove duplicate and missing data\n",
    "df_fashion = df_fashion.drop_duplicates(subset=['reviewerID', 'asin']).dropna(subset=['reviewText'])\n",
    "df_beauty = df_beauty.drop_duplicates(subset=['reviewerID', 'asin']).dropna(subset=['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3026 reviews in Amazon Fashion\n",
      "There are 4088 reviews in Amazon Beauty\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(df_fashion), \"reviews in Amazon Fashion\")\n",
    "print(\"There are\", len(df_beauty), \"reviews in Amazon Beauty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html.parser\n",
      "Requirement already satisfied: ply in /anaconda3/lib/python3.7/site-packages (from html.parser) (3.11)\n",
      "Installing collected packages: html.parser\n",
      "Successfully installed html.parser\n",
      "Requirement already satisfied: pattern3 in /anaconda3/lib/python3.7/site-packages (3.0.0)\n",
      "Requirement already satisfied: simplejson in /anaconda3/lib/python3.7/site-packages (from pattern3) (3.17.0)\n",
      "Requirement already satisfied: pdfminer.six in /anaconda3/lib/python3.7/site-packages (from pattern3) (20200124)\n",
      "Requirement already satisfied: docx in /anaconda3/lib/python3.7/site-packages (from pattern3) (0.2.4)\n",
      "Requirement already satisfied: pdfminer3k in /anaconda3/lib/python3.7/site-packages (from pattern3) (1.3.1)\n",
      "Requirement already satisfied: feedparser in /anaconda3/lib/python3.7/site-packages (from pattern3) (5.2.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /anaconda3/lib/python3.7/site-packages (from pattern3) (4.7.1)\n",
      "Requirement already satisfied: cherrypy in /anaconda3/lib/python3.7/site-packages (from pattern3) (18.5.0)\n",
      "Requirement already satisfied: sortedcontainers in /anaconda3/lib/python3.7/site-packages (from pdfminer.six->pattern3) (2.1.0)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /anaconda3/lib/python3.7/site-packages (from pdfminer.six->pattern3) (3.0.4)\n",
      "Requirement already satisfied: pycryptodome in /anaconda3/lib/python3.7/site-packages (from pdfminer.six->pattern3) (3.9.7)\n",
      "Requirement already satisfied: Pillow>=2.0 in /anaconda3/lib/python3.7/site-packages (from docx->pattern3) (6.1.0)\n",
      "Requirement already satisfied: lxml in /anaconda3/lib/python3.7/site-packages (from docx->pattern3) (4.3.4)\n",
      "Requirement already satisfied: pytest>=2.0 in /anaconda3/lib/python3.7/site-packages (from pdfminer3k->pattern3) (5.0.1)\n",
      "Requirement already satisfied: ply>=3.4 in /anaconda3/lib/python3.7/site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /anaconda3/lib/python3.7/site-packages (from beautifulsoup4->pattern3) (1.8)\n",
      "Requirement already satisfied: jaraco.collections in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: portend>=2.1.1 in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (2.6)\n",
      "Requirement already satisfied: more-itertools in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (7.0.0)\n",
      "Requirement already satisfied: zc.lockfile in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (2.0)\n",
      "Requirement already satisfied: cheroot>=8.2.1 in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (8.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /anaconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (1.8.0)\n",
      "Requirement already satisfied: packaging in /anaconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (19.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /anaconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (19.1.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /anaconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (1.3.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /anaconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /anaconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.17)\n",
      "Requirement already satisfied: wcwidth in /anaconda3/lib/python3.7/site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.1.7)\n",
      "Requirement already satisfied: jaraco.classes in /anaconda3/lib/python3.7/site-packages (from jaraco.collections->cherrypy->pattern3) (3.1.0)\n",
      "Requirement already satisfied: six>=1.7.0 in /anaconda3/lib/python3.7/site-packages (from jaraco.collections->cherrypy->pattern3) (1.12.0)\n",
      "Requirement already satisfied: jaraco.text in /anaconda3/lib/python3.7/site-packages (from jaraco.collections->cherrypy->pattern3) (3.2.0)\n",
      "Requirement already satisfied: tempora>=1.8 in /anaconda3/lib/python3.7/site-packages (from portend>=2.1.1->cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from zc.lockfile->cherrypy->pattern3) (41.0.1)\n",
      "Requirement already satisfied: jaraco.functools in /anaconda3/lib/python3.7/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda3/lib/python3.7/site-packages (from packaging->pytest>=2.0->pdfminer3k->pattern3) (2.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest>=2.0->pdfminer3k->pattern3) (0.5.1)\n",
      "Requirement already satisfied: pytz in /anaconda3/lib/python3.7/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2019.1)\n",
      "Requirement already satisfied: pyLDAvis in /anaconda3/lib/python3.7/site-packages (2.1.2)\n",
      "Requirement already satisfied: pytest in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (5.0.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.16.4)\n",
      "Requirement already satisfied: funcy in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.14)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.17.1)\n",
      "Requirement already satisfied: numexpr in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.6.9)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.3.1)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.33.4)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.25.3)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.10.1)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.13.2)\n",
      "Requirement already satisfied: py>=1.5.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.8.0)\n",
      "Requirement already satisfied: packaging in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (7.0.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.17)\n",
      "Requirement already satisfied: wcwidth in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.1.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /anaconda3/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda3/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (2.4.0)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->pyLDAvis) (0.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fortunagd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/fortunagd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fortunagd/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fortunagd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
     ]
    }
   ],
   "source": [
    "%run ./Text_Normalization_Function.ipynb #defining text normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess review data\n",
    "df_fashion_reviews = normalize_corpus(df_fashion['reviewText'])\n",
    "df_beauty_reviews = normalize_corpus(df_beauty['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most important k features, The classes in the sklearn.feature_selection module can be used for feature selection\n",
    "# /dimensionality reduction on sample sets, either to improve estimators’ accuracy scores or to boost their performance on \n",
    "# very high-dimensional datasets.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "\n",
    "def find_k_best_features(corpus, k):\n",
    "    vectorizer = CountVectorizer()\n",
    "    corpus_train = vectorizer.fit_transform(corpus)\n",
    "    corpus_train_table = pd.DataFrame(data = corpus_train.todense(), columns = vectorizer.get_feature_names())\n",
    "    chi2_kbest = SelectKBest(score_func = chi2, k = k)\n",
    "    NORM_corpus_train_chi2_BEST = chi2_kbest.fit_transform(corpus_train, corpus)\n",
    "    chi2_best_features_ind = chi2_kbest.get_support(indices=True)\n",
    "    chi2_best_features_names = np.array(vectorizer.get_feature_names())[chi2_best_features_ind]\n",
    "    X_train_bow_chi2_BEST_table = pd.DataFrame(data = NORM_corpus_train_chi2_BEST.todense(), columns = chi2_best_features_names)\n",
    "    return X_train_bow_chi2_BEST_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st</th>\n",
       "      <th>capri</th>\n",
       "      <th>day</th>\n",
       "      <th>deep</th>\n",
       "      <th>fault</th>\n",
       "      <th>finish</th>\n",
       "      <th>fruit</th>\n",
       "      <th>glass</th>\n",
       "      <th>gray</th>\n",
       "      <th>grommet</th>\n",
       "      <th>hip</th>\n",
       "      <th>loom</th>\n",
       "      <th>male</th>\n",
       "      <th>offer</th>\n",
       "      <th>pain</th>\n",
       "      <th>ring</th>\n",
       "      <th>show</th>\n",
       "      <th>teal</th>\n",
       "      <th>thigh</th>\n",
       "      <th>wear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3026 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1st  capri  day  deep  fault  finish  fruit  glass  gray  grommet  hip  \\\n",
       "0       0      0    0     0      0       0      0      0     0        0    0   \n",
       "1       0      0    0     0      0       0      0      0     0        0    0   \n",
       "2       0      0    1     0      0       0      0      0     0        0    0   \n",
       "3       0      0    1     0      0       0      0      0     0        0    0   \n",
       "4       0      0    0     0      0       0      0      0     0        0    0   \n",
       "...   ...    ...  ...   ...    ...     ...    ...    ...   ...      ...  ...   \n",
       "3021    0      0    0     0      0       0      0      0     0        0    0   \n",
       "3022    0      0    0     0      0       0      0      0     0        0    0   \n",
       "3023    0      0    0     0      0       0      0      0     0        0    0   \n",
       "3024    0      0    1     0      0       0      0      0     0        0    0   \n",
       "3025    0      0    0     0      0       0      0      0     0        0    0   \n",
       "\n",
       "      loom  male  offer  pain  ring  show  teal  thigh  wear  \n",
       "0        0     0      0     0     0     0     0      0     0  \n",
       "1        0     0      0     0     0     0     0      0     0  \n",
       "2        0     0      0     1     0     0     0      0     0  \n",
       "3        0     0      0     0     0     0     0      0     0  \n",
       "4        0     0      0     0     0     0     0      0     0  \n",
       "...    ...   ...    ...   ...   ...   ...   ...    ...   ...  \n",
       "3021     0     0      0     0     0     0     0      0     0  \n",
       "3022     0     0      0     0     0     0     0      0     0  \n",
       "3023     0     0      0     0     0     0     0      0     0  \n",
       "3024     0     0      0     1     0     0     0      0     1  \n",
       "3025     0     0      0     0     0     0     0      0     1  \n",
       "\n",
       "[3026 rows x 20 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_k_best_features(df_fashion_reviews, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aqua</th>\n",
       "      <th>calibra</th>\n",
       "      <th>certified</th>\n",
       "      <th>exfoliating</th>\n",
       "      <th>exfoliator</th>\n",
       "      <th>eye</th>\n",
       "      <th>gum</th>\n",
       "      <th>hibiscus</th>\n",
       "      <th>mouthwash</th>\n",
       "      <th>odor</th>\n",
       "      <th>patchouli</th>\n",
       "      <th>polish</th>\n",
       "      <th>pump</th>\n",
       "      <th>shave</th>\n",
       "      <th>sonicare</th>\n",
       "      <th>toothbrush</th>\n",
       "      <th>toothpaste</th>\n",
       "      <th>tweezer</th>\n",
       "      <th>vanilla</th>\n",
       "      <th>velva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4086</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4087</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4088 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aqua  calibra  certified  exfoliating  exfoliator  eye  gum  hibiscus  \\\n",
       "0        0        0          0            0           0    0    0         0   \n",
       "1        0        0          0            0           0    0    0         0   \n",
       "2        0        0          0            0           0    0    0         0   \n",
       "3        5        0          0            0           0    0    0         0   \n",
       "4        0        0          0            0           0    0    0         0   \n",
       "...    ...      ...        ...          ...         ...  ...  ...       ...   \n",
       "4083     0        0          0            0           0    0    0         0   \n",
       "4084     0        0          0            0           0    0    0         0   \n",
       "4085     0        0          0            0           0    0    0         0   \n",
       "4086     0        0          0            0           0    0    0         0   \n",
       "4087     0        0          0            0           0    0    0         0   \n",
       "\n",
       "      mouthwash  odor  patchouli  polish  pump  shave  sonicare  toothbrush  \\\n",
       "0             0     0          0       0     0      0         0           0   \n",
       "1             0     0          0       0     0      0         0           0   \n",
       "2             0     0          0       0     0      1         0           0   \n",
       "3             0     1          0       0     0      5         0           0   \n",
       "4             0     0          0       0     0      0         0           0   \n",
       "...         ...   ...        ...     ...   ...    ...       ...         ...   \n",
       "4083          0     0          0       0     0      0         0           0   \n",
       "4084          0     0          0       0     0      0         0           0   \n",
       "4085          0     0          0       0     0      0         0           0   \n",
       "4086          0     0          0       0     0      0         0           0   \n",
       "4087          0     0          0       0     0      0         0           0   \n",
       "\n",
       "      toothpaste  tweezer  vanilla  velva  \n",
       "0              0        0        0      0  \n",
       "1              0        0        0      0  \n",
       "2              0        0        0      0  \n",
       "3              0        0        0      5  \n",
       "4              0        0        0      0  \n",
       "...          ...      ...      ...    ...  \n",
       "4083           0        0        0      0  \n",
       "4084           0        0        0      0  \n",
       "4085           0        0        0      0  \n",
       "4086           0        0        0      0  \n",
       "4087           0        0        0      0  \n",
       "\n",
       "[4088 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_k_best_features(df_beauty_reviews, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "shoe fit comfortable size great love like light foot perfect\n",
      "Topic 1:\n",
      "shoe love wear foot comfortable day good use look nike\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el24101121640632969213281172\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el24101121640632969213281172_data = {\"mdsDat\": {\"x\": [-60.31231689453125, 60.31256103515625], \"y\": [141.36962890625, -141.369140625], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [52.40238044310782, 47.59761955689219]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [633.0, 880.0, 208.0, 191.0, 275.0, 601.0, 105.0, 317.0, 104.0, 99.0, 80.0, 190.0, 217.0, 76.0, 981.0, 166.0, 67.0, 65.0, 64.0, 214.0, 62.0, 71.0, 965.0, 459.0, 693.0, 56.0, 88.0, 51.0, 57.0, 57.0, 207.89810317066917, 191.25300389833015, 105.05268237060251, 104.07209592046544, 99.17668752863729, 70.77193143196033, 57.05650391580489, 57.05546853421472, 47.26288487058979, 45.30383677502871, 47.24974828150411, 39.42077796445152, 39.40819899249378, 31.590227906537304, 31.590224711028572, 31.59021976155262, 31.585514149276083, 30.606098249513487, 29.623776812368288, 26.687148615539996, 23.753893957207378, 23.753889204843844, 23.753888632424996, 23.75388522116279, 23.75388014316628, 23.753831228098544, 23.7501039804286, 23.75009575736208, 23.748898585221713, 23.74889660678651, 24.709161437511586, 566.9066604270048, 171.1070666098572, 718.7830233525078, 190.77414094841018, 487.5043588484846, 267.8031834072133, 86.8176329068788, 93.18467473230005, 181.8677829474311, 694.9888098754758, 346.68508497244096, 111.53920042245196, 191.2185175462223, 106.70299545909722, 131.16497303296754, 306.14451094602737, 1165.4055631451538, 191.91825288017304, 155.20448204734888, 185.7915965282048, 186.74407931246483, 112.92138992582065, 212.13255245983206, 290.49442669386974, 382.9146822177353, 217.6931446782069, 189.82588246840794, 260.6745677499921, 176.0868755099878, 175.06415381229658, 142.42626207929948, 126.07877347682394, 131.94387611071787, 126.64388076407488, 80.7172759645294, 76.74321314280454, 66.81370618065516, 64.82738239969713, 63.830660006831394, 61.84283725510159, 55.88366019036372, 50.916932301551164, 47.93123175790353, 45.94942372854713, 40.97979899493927, 39.98930130722177, 39.986964569664494, 39.98673420106563, 33.034622582683845, 33.034465998414056, 33.03386192248013, 32.04112317974066, 32.04110466683238, 31.04225395296157, 29.05492340568728, 28.06362507899206, 26.079437300488234, 26.07938087259481, 26.074258587171894, 26.07125519240717, 25.086454419376786, 25.08032250802685, 24.09293506367333, 24.092933541343324, 239.3351660390332, 80.4336592591601, 71.78613128376485, 138.8837335584714, 88.22686585518888, 582.7891056965557, 79.58313401634703, 62.75513235597103, 432.5614882553926, 56.04565239978272, 212.5809826359667, 77.44992804438704, 72.8547788737649, 923.7335819446838, 87.33411209334405, 72.28638834667302, 229.28499544719406, 295.01550848328895, 184.01164348573326, 186.51969966092335, 180.532786902806, 137.92105476429595, 188.0346228009053, 101.8892783528709, 122.3821120265767, 286.1313211499595, 158.9012650692487, 102.9515503295513, 104.53997927962584, 121.30841242190894, 114.05699095140638, 127.13299657501203, 161.65165158130702, 91.34429904733202, 112.83892962929237, 114.07066620548383, 91.0634190998325, 87.28681878370139], \"Term\": [\"size\", \"fit\", \"expect\", \"big\", \"day\", \"great\", \"small\", \"perfect\", \"large\", \"half\", \"daughter\", \"super\", \"order\", \"new\", \"comfortable\", \"gym\", \"white\", \"black\", \"hour\", \"nice\", \"overall\", \"narrow\", \"love\", \"like\", \"wear\", \"awesome\", \"year\", \"problem\", \"sock\", \"hard\", \"expect\", \"big\", \"small\", \"large\", \"half\", \"narrow\", \"sock\", \"hard\", \"exactly\", \"room\", \"normally\", \"compare\", \"brand\", \"comfortably\", \"amazing\", \"otherwise\", \"athletic\", \"usual\", \"glad\", \"inch\", \"plus\", \"bad\", \"boot\", \"attractive\", \"supportive\", \"wrong\", \"plenty\", \"pleased\", \"satisfied\", \"description\", \"snug\", \"size\", \"super\", \"fit\", \"order\", \"great\", \"perfect\", \"heel\", \"true\", \"nice\", \"comfortable\", \"like\", \"well\", \"feel\", \"cute\", \"comfy\", \"light\", \"shoe\", \"weight\", \"toe\", \"really\", \"time\", \"wide\", \"run\", \"foot\", \"love\", \"look\", \"pair\", \"wear\", \"nike\", \"good\", \"buy\", \"lightweight\", \"color\", \"support\", \"daughter\", \"new\", \"white\", \"black\", \"hour\", \"overall\", \"awesome\", \"problem\", \"cardio\", \"know\", \"inside\", \"mile\", \"exercise\", \"wife\", \"month\", \"excellent\", \"hot\", \"hip\", \"insole\", \"hold\", \"pant\", \"insert\", \"quick\", \"dark\", \"ive\", \"provide\", \"stand\", \"photo\", \"grommet\", \"lift\", \"day\", \"year\", \"best\", \"gym\", \"comfort\", \"love\", \"lot\", \"around\", \"wear\", \"pink\", \"use\", \"take\", \"pain\", \"shoe\", \"much\", \"favorite\", \"good\", \"foot\", \"support\", \"nike\", \"pair\", \"work\", \"look\", \"workout\", \"training\", \"comfortable\", \"run\", \"walk\", \"long\", \"buy\", \"color\", \"light\", \"fit\", \"purchase\", \"like\", \"great\", \"sneaker\", \"time\"], \"Total\": [633.0, 880.0, 208.0, 191.0, 275.0, 601.0, 105.0, 317.0, 104.0, 99.0, 80.0, 190.0, 217.0, 76.0, 981.0, 166.0, 67.0, 65.0, 64.0, 214.0, 62.0, 71.0, 965.0, 459.0, 693.0, 56.0, 88.0, 51.0, 57.0, 57.0, 208.15643877512738, 191.50411280015976, 105.30437174262482, 104.32484403621585, 99.4270981188317, 71.02033432662355, 57.3067639927808, 57.306778770411, 47.511312153686596, 45.552226414293095, 47.51149964751264, 39.67505361199088, 39.67523314714786, 31.83863012354438, 31.838630169152708, 31.838630239794778, 31.838697401311496, 30.859152988056742, 29.879650044495367, 26.94099369015335, 24.0022891865061, 24.002289254334862, 24.00228926250479, 24.00228931119249, 24.0022893836689, 24.00229008181592, 24.00234327948564, 24.0023433968505, 24.002360483659174, 24.002360511896665, 24.982178261494234, 633.2280122153657, 190.79812527868287, 880.4346749338149, 217.34255234075437, 601.5750250539685, 317.58251749415433, 95.62940141504623, 103.4867130320729, 214.48909980075408, 981.1201310254353, 459.5240146017333, 132.03692016154312, 249.1289544067239, 128.13185252027307, 162.55602977011856, 433.2775075210394, 2089.1391450898377, 260.0477232548243, 204.93442288580826, 258.14812029468226, 274.0308980961662, 143.93947185126422, 371.03381752908075, 585.5099351771587, 965.703787914291, 405.72776747911223, 370.358669371214, 693.2360560053846, 362.6065751709111, 404.3491492594907, 263.7346745012084, 197.4019375640492, 246.00086706212426, 310.65552424980814, 80.96855965092175, 76.9944670147141, 67.05931493116212, 65.07227857068477, 64.07871028776789, 62.091652834073436, 56.13054085776422, 51.1629370325172, 48.182295037361236, 46.19532222163812, 41.22767763256978, 40.234196942460066, 40.23416406035604, 40.23416081865486, 33.27953386114593, 33.279531657722664, 33.27952315728758, 32.28601093149938, 32.286010670989356, 31.292412438764632, 29.305361911272783, 28.31186995473131, 26.324863651653516, 26.324862857611716, 26.32479077783483, 26.324748514663874, 25.331347990399134, 25.33126170336793, 24.337824779981254, 24.337824758559325, 275.1909494104192, 88.80090949209936, 79.86333976517358, 166.02773041972267, 104.58325561338654, 965.703787914291, 95.64573930493489, 72.8794591214121, 693.2360560053846, 63.96916121842699, 313.996174088382, 95.61572125733262, 88.69426100378088, 2089.1391450898377, 112.40703379979041, 88.68626272601333, 404.3491492594907, 585.5099351771587, 310.65552424980814, 362.6065751709111, 370.358669371214, 250.2548478124724, 405.72776747911223, 159.6298962192906, 225.5476217985032, 981.1201310254353, 371.03381752908075, 172.37889816089708, 190.0330171023469, 263.7346745012084, 246.00086706212426, 433.2775075210394, 880.4346749338149, 183.97007450142166, 459.5240146017333, 601.5750250539685, 187.88429243435624, 274.0308980961662], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.645, 0.6449, 0.6438, 0.6438, 0.6437, 0.6427, 0.6418, 0.6418, 0.641, 0.6408, 0.6407, 0.6398, 0.6395, 0.6384, 0.6384, 0.6384, 0.6382, 0.638, 0.6376, 0.6368, 0.6358, 0.6358, 0.6358, 0.6358, 0.6358, 0.6358, 0.6357, 0.6357, 0.6356, 0.6356, 0.6352, 0.5356, 0.5373, 0.4434, 0.5158, 0.436, 0.4757, 0.5495, 0.5414, 0.4812, 0.3014, 0.3644, 0.4775, 0.3817, 0.4632, 0.4317, 0.2989, 0.0625, 0.3424, 0.3683, 0.3173, 0.2627, 0.4035, 0.0871, -0.0547, -0.2788, 0.0236, -0.0221, -0.3319, -0.0761, -0.1909, 0.0301, 0.1979, 0.0233, -0.2511, 0.7393, 0.7391, 0.7387, 0.7386, 0.7385, 0.7384, 0.738, 0.7376, 0.7372, 0.7371, 0.7364, 0.7363, 0.7362, 0.7362, 0.735, 0.735, 0.735, 0.7348, 0.7348, 0.7344, 0.7338, 0.7336, 0.733, 0.733, 0.7328, 0.7327, 0.7327, 0.7324, 0.7323, 0.7323, 0.6028, 0.6434, 0.6358, 0.5639, 0.5723, 0.2374, 0.5585, 0.5928, 0.2707, 0.6102, 0.3523, 0.5317, 0.5457, -0.0737, 0.49, 0.5379, 0.1751, 0.0569, 0.2187, 0.0776, 0.0238, 0.1466, -0.0267, 0.2934, 0.131, -0.4899, -0.1056, 0.227, 0.1448, -0.0342, -0.0262, -0.4838, -0.9526, 0.0423, -0.6618, -0.9203, 0.0181, -0.4017], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.4967, -4.5801, -5.1793, -5.1887, -5.2368, -5.5743, -5.7897, -5.7897, -5.978, -6.0204, -5.9783, -6.1594, -6.1598, -6.3809, -6.3809, -6.3809, -6.381, -6.4125, -6.4452, -6.5496, -6.666, -6.666, -6.666, -6.666, -6.666, -6.666, -6.6662, -6.6662, -6.6662, -6.6662, -6.6266, -3.4935, -4.6915, -3.2562, -4.5827, -3.6444, -4.2435, -5.3699, -5.2992, -4.6305, -3.2898, -3.9853, -5.1194, -4.5803, -5.1637, -4.9573, -4.1097, -2.7729, -4.5767, -4.789, -4.6091, -4.604, -5.1071, -4.4765, -4.1622, -3.8859, -4.4507, -4.5876, -4.2705, -4.6628, -4.6686, -4.8749, -4.9968, -4.9514, -4.9924, -5.3466, -5.3971, -5.5357, -5.5658, -5.5813, -5.613, -5.7143, -5.8074, -5.8678, -5.91, -6.0245, -6.049, -6.049, -6.049, -6.24, -6.24, -6.24, -6.2706, -6.2706, -6.3022, -6.3684, -6.4031, -6.4764, -6.4764, -6.4766, -6.4767, -6.5152, -6.5155, -6.5557, -6.5557, -4.2597, -5.3501, -5.4639, -4.8039, -5.2577, -3.3697, -5.3608, -5.5983, -3.6678, -5.7114, -4.3783, -5.3879, -5.4491, -2.9091, -5.2678, -5.4569, -4.3026, -4.0505, -4.5226, -4.509, -4.5417, -4.8109, -4.5009, -5.1137, -4.9304, -4.0811, -4.6693, -5.1033, -5.088, -4.9392, -5.0009, -4.8923, -4.6521, -5.2229, -5.0116, -5.0008, -5.226, -5.2684]}, \"token.table\": {\"Topic\": [1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2], \"Freq\": [1.0050683660066393, 0.1372128734289959, 0.8644411026026743, 1.0050662436548632, 0.9999046211316425, 0.9976743345820414, 0.9999046235002584, 0.10017111760568523, 0.901540058451167, 0.9973676137144594, 0.9988892571111329, 0.9999046231599098, 0.9829809910720991, 0.5384199111040644, 0.4587944312929, 0.9962165555372594, 0.5365834745885881, 0.4634130007810534, 0.15298816150022412, 0.8414348882512326, 0.7083740084648027, 0.29150354880709867, 1.0050683674463836, 0.8058759812555457, 0.19070347648032, 0.9829854391983263, 0.83507728870985, 0.16389367348511075, 0.9876594662859646, 1.0003883031785894, 0.13081825574979095, 0.8684878645611122, 0.9999016550103272, 0.9892380965604015, 0.9916004930418605, 0.9941799695401955, 0.9992484557477639, 0.1804112554548643, 0.8118506495468893, 0.7666712223589094, 0.23281115652783635, 0.8166420751818407, 0.18400002250272351, 0.4952947551816593, 0.5038343199261707, 1.004027823462638, 0.43279428266508835, 0.5663422327446013, 0.8112038892509219, 0.18950254789878093, 0.9861193519537897, 0.16262343604735943, 0.8372095411327022, 0.995704409291708, 0.9946467280661498, 0.9097620471596041, 0.09411331522340732, 0.991141335728771, 0.9906554843179046, 0.9916007463218004, 0.9987716624224424, 1.0021902053994474, 0.9889844805295458, 0.9944775537783406, 0.9911413437261126, 0.9876621705913688, 0.995771818178884, 0.9968862255274201, 0.9861193528217629, 0.7062448308262137, 0.2931146846893109, 0.6382916072397614, 0.35967225487319887, 0.7551291966770067, 0.2459066260072097, 0.44729069345997485, 0.5525355625093807, 0.537306089140727, 0.46336488421310396, 0.16728398061715302, 0.8364199030857652, 0.3966019443987025, 0.6037047874267456, 0.9941791570291562, 0.9916004273884291, 0.22240601103777738, 0.7739729184114652, 0.9997136830343542, 1.0000718621155575, 0.8485279679436658, 0.1538539722095658, 0.48537454103540206, 0.5157104498501147, 0.9892341927468623, 0.8787970783583422, 0.12422785924437298, 1.0050683637766404, 0.9985239105436868, 0.18039498631504408, 0.8230521250623887, 0.5130162075659722, 0.48871543983916305, 0.9895799986296937, 0.8438751670420052, 0.15743939683619498, 0.9869228107448005, 0.12506026103239753, 0.8754218272267826, 0.9999023679975012, 0.9999023728867489, 0.9999046263259178, 0.9968153307458943, 0.9876637562374821, 0.5055169991752182, 0.49464566585962205, 0.9876594364950069, 0.7205165770243709, 0.27890964271911134, 0.9878770708313871, 0.5713764891077184, 0.4285323668307888, 0.9999016561866579, 0.5576459580196619, 0.4422874379486417, 0.8954120617885094, 0.10422785904416512, 0.9971096001278205, 0.516275196522297, 0.48434064828380446, 1.0007133780857387, 0.9946469845545729, 0.986919448956103, 0.8962352211282715, 0.1048228328804996, 0.4088129458077018, 0.5922959214851742, 0.9999046181123682, 0.18825356085069125, 0.8053068991946237, 0.6824047992368208, 0.3174824467037616, 0.7563395051809706, 0.24398048554224858, 0.45666630921968576, 0.5409057254835113, 0.8986660922468102, 0.0966307626071839, 0.3216599701994173, 0.6783522143809493, 1.0045641891725858, 0.4002810131411558, 0.5975209326599862, 0.37649513140437796, 0.624606865509945, 0.7383260179973066, 0.2614904647073794, 0.848247595164833, 0.15147278485086302, 0.9991154855783568, 0.7850522066439521, 0.21536830447754438, 0.9941800496421368, 0.44754377778897936, 0.551437869061421, 0.36334046048819607, 0.6389780512033794, 0.9999045890284587, 0.09008916739430199, 0.9008916739430198], \"Term\": [\"amazing\", \"around\", \"around\", \"athletic\", \"attractive\", \"awesome\", \"bad\", \"best\", \"best\", \"big\", \"black\", \"boot\", \"brand\", \"buy\", \"buy\", \"cardio\", \"color\", \"color\", \"comfort\", \"comfort\", \"comfortable\", \"comfortable\", \"comfortably\", \"comfy\", \"comfy\", \"compare\", \"cute\", \"cute\", \"dark\", \"daughter\", \"day\", \"day\", \"description\", \"exactly\", \"excellent\", \"exercise\", \"expect\", \"favorite\", \"favorite\", \"feel\", \"feel\", \"fit\", \"fit\", \"foot\", \"foot\", \"glad\", \"good\", \"good\", \"great\", \"great\", \"grommet\", \"gym\", \"gym\", \"half\", \"hard\", \"heel\", \"heel\", \"hip\", \"hold\", \"hot\", \"hour\", \"inch\", \"insert\", \"inside\", \"insole\", \"ive\", \"know\", \"large\", \"lift\", \"light\", \"light\", \"lightweight\", \"lightweight\", \"like\", \"like\", \"long\", \"long\", \"look\", \"look\", \"lot\", \"lot\", \"love\", \"love\", \"mile\", \"month\", \"much\", \"much\", \"narrow\", \"new\", \"nice\", \"nice\", \"nike\", \"nike\", \"normally\", \"order\", \"order\", \"otherwise\", \"overall\", \"pain\", \"pain\", \"pair\", \"pair\", \"pant\", \"perfect\", \"perfect\", \"photo\", \"pink\", \"pink\", \"pleased\", \"plenty\", \"plus\", \"problem\", \"provide\", \"purchase\", \"purchase\", \"quick\", \"really\", \"really\", \"room\", \"run\", \"run\", \"satisfied\", \"shoe\", \"shoe\", \"size\", \"size\", \"small\", \"sneaker\", \"sneaker\", \"snug\", \"sock\", \"stand\", \"super\", \"super\", \"support\", \"support\", \"supportive\", \"take\", \"take\", \"time\", \"time\", \"toe\", \"toe\", \"training\", \"training\", \"true\", \"true\", \"use\", \"use\", \"usual\", \"walk\", \"walk\", \"wear\", \"wear\", \"weight\", \"weight\", \"well\", \"well\", \"white\", \"wide\", \"wide\", \"wife\", \"work\", \"work\", \"workout\", \"workout\", \"wrong\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el24101121640632969213281172\", ldavis_el24101121640632969213281172_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el24101121640632969213281172\", ldavis_el24101121640632969213281172_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el24101121640632969213281172\", ldavis_el24101121640632969213281172_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=               x           y  topics  cluster      Freq\n",
       "topic                                                  \n",
       "0     -60.312317  141.369629       1        1  52.40238\n",
       "1      60.312561 -141.369141       2        1  47.59762, topic_info=    Category        Freq      Term       Total  loglift  logprob\n",
       "761  Default  633.000000      size  633.000000  30.0000  30.0000\n",
       "316  Default  880.000000       fit  880.000000  29.0000  29.0000\n",
       "287  Default  208.000000    expect  208.000000  28.0000  28.0000\n",
       "91   Default  191.000000       big  191.000000  27.0000  27.0000\n",
       "212  Default  275.000000       day  275.000000  26.0000  26.0000\n",
       "..       ...         ...       ...         ...      ...      ...\n",
       "658   Topic2   91.344299  purchase  183.970075   0.0423  -5.2229\n",
       "463   Topic2  112.838930      like  459.524015  -0.6618  -5.0116\n",
       "358   Topic2  114.070666     great  601.575025  -0.9203  -5.0008\n",
       "776   Topic2   91.063419   sneaker  187.884292   0.0181  -5.2260\n",
       "880   Topic2   87.286819      time  274.030898  -0.4017  -5.2684\n",
       "\n",
       "[163 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "40        1  1.005068     amazing\n",
       "57        1  0.137213      around\n",
       "57        2  0.864441      around\n",
       "62        1  1.005066    athletic\n",
       "64        1  0.999905  attractive\n",
       "...     ...       ...         ...\n",
       "984       1  0.363340     workout\n",
       "984       2  0.638978     workout\n",
       "990       1  0.999905       wrong\n",
       "992       1  0.090089        year\n",
       "992       2  0.900892        year\n",
       "\n",
       "[169 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df_fashion_reviews\n",
    "#define a Bag-of-Words vecgtorizer\n",
    "bow_vectorizer_news = CountVectorizer(max_features=1000)\n",
    "\n",
    "#vectorize data\n",
    "bow_news_corpus = bow_vectorizer_news.fit_transform(corpus)\n",
    "    \n",
    "lda_news = LatentDirichletAllocation(n_components=2, max_iter=100,\n",
    "                                     doc_topic_prior = 0.25,\n",
    "                                     topic_word_prior = 0.25).fit(bow_news_corpus)\n",
    "no_top_words_news = 10\n",
    "display_topics(lda_news, bow_vectorizer_news.get_feature_names(), no_top_words_news)\n",
    "#prepare to display result in the Jupyter notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#run the visualization [mds is a function to use for visualizing the \"distance\" between topics]\n",
    "pyLDAvis.sklearn.prepare(lda_news, bow_news_corpus, bow_vectorizer_news, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "love smell product great scent body skin like buy soap\n",
      "Topic 1:\n",
      "use hair product shampoo good like well work great really\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2410112171632704787172338\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2410112171632704787172338_data = {\"mdsDat\": {\"x\": [-60.31231689453125, 60.31256103515625], \"y\": [141.36962890625, -141.369140625], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [54.7891878200838, 45.2108121799162]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [1408.0, 1242.0, 636.0, 549.0, 953.0, 668.0, 386.0, 362.0, 282.0, 357.0, 258.0, 256.0, 1063.0, 265.0, 200.0, 1666.0, 639.0, 177.0, 177.0, 550.0, 333.0, 219.0, 139.0, 131.0, 205.0, 158.0, 163.0, 458.0, 232.0, 136.0, 1241.8884619336761, 668.4318920470229, 265.524042807573, 157.82202611842942, 135.87925298141255, 129.89683738378548, 131.8741593526976, 124.90651194295431, 120.9127996372149, 101.97362007021871, 104.95443403618087, 99.978182199629, 84.02237237483662, 84.01467578028775, 78.0379713857058, 78.03582121406474, 81.01488167145787, 74.04932184285293, 74.04227225317099, 75.03489041735087, 65.06989481050357, 65.06624496630737, 66.05787415323212, 65.06450099679435, 63.07700967277083, 63.07362018516569, 64.06579194413555, 63.069398141222294, 62.07496367291916, 63.06344892666711, 63.06261889249811, 87.89115799207484, 219.2063819236441, 129.48529612927797, 1308.7851980882058, 339.7662674277543, 287.9883456608732, 278.97840503778224, 342.1319262559215, 295.92042090985717, 124.41650735015358, 154.03090683794662, 473.20044501783684, 171.01941703523943, 786.3087991526784, 133.14091739319028, 291.00308660807514, 453.6917530049956, 223.55115931508436, 135.82829235657152, 175.83971308093658, 238.0021247073792, 220.4341080891275, 230.6925426399002, 138.89086996839563, 179.77754856210834, 325.503130039989, 204.08939517017652, 172.08367416232144, 149.71806011627726, 147.3742001165931, 165.44104715002982, 146.24884348948044, 636.6149389080101, 548.9341955886161, 386.4001757456567, 361.7666009176696, 281.956249475726, 258.3332613790189, 256.35010223480697, 200.21850762891947, 177.55510279570336, 177.54390332606346, 139.141291721293, 131.26051421966548, 90.86426153428287, 87.91125312944392, 81.0159742847751, 81.99704251827484, 74.11863192466018, 69.19899665837058, 65.25854460033814, 53.43229246226151, 59.305790766813416, 50.47919292243781, 53.41660405651443, 56.352253315336824, 47.52922195313501, 47.52311264571758, 44.562146198883525, 42.60146478183815, 41.61874012500946, 41.61869705564491, 1350.383549237195, 336.7712370301791, 830.0387324880545, 152.06768096329253, 199.70137380571197, 98.45166753095977, 184.09483417994898, 274.0808401487801, 84.70962383693353, 474.43631028061213, 737.9560034659455, 404.73456702206914, 116.38677917395367, 327.16262260452066, 794.055926904939, 179.79838550821106, 436.9728043313307, 201.39115306696678, 145.83773314950884, 260.91276352449324, 180.93718568735565, 204.8528662809119, 269.93758159663764, 357.76522054172233, 177.49921977967455, 178.0177614411533], \"Term\": [\"love\", \"hair\", \"scent\", \"body\", \"smell\", \"shampoo\", \"soap\", \"fragrance\", \"favorite\", \"shower\", \"lotion\", \"gel\", \"great\", \"conditioner\", \"bath\", \"use\", \"skin\", \"lavender\", \"amazon\", \"buy\", \"price\", \"store\", \"bar\", \"perfume\", \"wonderful\", \"brush\", \"wish\", \"wash\", \"water\", \"scalp\", \"hair\", \"shampoo\", \"conditioner\", \"brush\", \"scalp\", \"eye\", \"shave\", \"fine\", \"less\", \"toothpaste\", \"face\", \"dandruff\", \"teeth\", \"away\", \"mouth\", \"thin\", \"top\", \"mouthwash\", \"hold\", \"job\", \"curly\", \"minute\", \"difference\", \"powder\", \"gum\", \"kid\", \"expect\", \"issue\", \"around\", \"end\", \"free\", \"head\", \"water\", \"start\", \"use\", \"work\", \"look\", \"dry\", \"well\", \"really\", \"easy\", \"help\", \"good\", \"even\", \"product\", \"without\", \"feel\", \"like\", \"much\", \"first\", \"give\", \"clean\", \"leave\", \"year\", \"keep\", \"recommend\", \"great\", \"time\", \"nice\", \"little\", \"soft\", \"skin\", \"buy\", \"scent\", \"body\", \"soap\", \"fragrance\", \"favorite\", \"lotion\", \"gel\", \"bath\", \"lavender\", \"amazon\", \"bar\", \"perfume\", \"sell\", \"thank\", \"thanks\", \"arrive\", \"pre\", \"de\", \"provence\", \"bring\", \"polish\", \"seller\", \"friend\", \"coat\", \"cologne\", \"lip\", \"woman\", \"christmas\", \"shipping\", \"pricey\", \"love\", \"shower\", \"smell\", \"wish\", \"store\", \"available\", \"wonderful\", \"price\", \"longer\", \"skin\", \"great\", \"buy\", \"gift\", \"wash\", \"product\", \"last\", \"like\", \"best\", \"hard\", \"time\", \"long\", \"nice\", \"good\", \"use\", \"year\", \"feel\"], \"Total\": [1408.0, 1242.0, 636.0, 549.0, 953.0, 668.0, 386.0, 362.0, 282.0, 357.0, 258.0, 256.0, 1063.0, 265.0, 200.0, 1666.0, 639.0, 177.0, 177.0, 550.0, 333.0, 219.0, 139.0, 131.0, 205.0, 158.0, 163.0, 458.0, 232.0, 136.0, 1242.1383133328634, 668.6887606037656, 265.77821283318326, 158.06952621961838, 136.12882533798177, 130.14502201424338, 132.13941737227563, 125.158458325286, 121.1691896095415, 102.22054192911467, 105.21231642291775, 100.22592492386057, 84.26908300704146, 84.26898883956144, 78.28525539212933, 78.28522908487805, 81.2769821245851, 74.29604861905536, 74.29596236764999, 75.2932084625775, 65.32027262622924, 65.3202279705538, 66.31746196539014, 65.32020663316679, 63.32568685354442, 63.32564538331951, 64.32288601649941, 63.325593726806346, 62.32832540948178, 63.32552093844085, 63.32551078301094, 89.2419247082962, 232.70317029583217, 137.03569703037383, 1666.5504186299281, 405.5858685494915, 351.75688936077887, 363.4678706484469, 472.6017114028226, 397.168604990081, 142.88428938142246, 185.60598286829784, 743.1380266144745, 215.36688023482247, 1580.3647260576174, 157.7675539997484, 469.02084804922845, 890.6645573363262, 339.1472858831466, 166.6663470275078, 243.99379860814392, 396.4599773238886, 375.5579028429936, 408.19176241957473, 172.614426268993, 282.46093469496316, 1063.4591335059345, 465.0021586946698, 376.93654044323335, 246.62950576962834, 275.1687710262054, 639.877357430642, 550.9834105115496, 636.871228125401, 549.1972815429478, 386.6554453810451, 362.0279837316189, 282.23497787588667, 258.59230201083557, 256.62225957767197, 200.4712388074133, 177.8139804812703, 177.81411920351835, 139.3949598906307, 131.51414780546122, 91.12507589393044, 88.16974308164806, 81.27402753432544, 82.25917894961906, 74.37833754674, 69.45275752414892, 65.51235226571919, 53.6911971342946, 59.602265549376945, 50.735865450856245, 53.69139145875712, 56.64693929013142, 47.780495015374996, 47.780570688320154, 44.82533644833736, 42.855015594772006, 41.86988469676874, 41.869885230247526, 1408.9187088220758, 357.3510717590912, 953.6124779614911, 163.17012919737664, 219.4264092437095, 103.99603891980026, 205.6574718127113, 333.1950010071219, 90.2040069064608, 639.877357430642, 1063.4591335059345, 550.9834105115496, 129.70377451477955, 458.1977341786094, 1580.3647260576174, 243.60821969667975, 890.6645573363262, 296.197842037475, 186.1852750900523, 465.0021586946698, 284.48355411700874, 376.93654044323335, 743.1380266144745, 1666.5504186299281, 408.19176241957473, 469.02084804922845], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6015, 0.6013, 0.6007, 0.6001, 0.5998, 0.5998, 0.5997, 0.5997, 0.5996, 0.5993, 0.5992, 0.5992, 0.5987, 0.5987, 0.5985, 0.5985, 0.5984, 0.5984, 0.5983, 0.5982, 0.5978, 0.5978, 0.5978, 0.5978, 0.5977, 0.5977, 0.5977, 0.5976, 0.5976, 0.5975, 0.5975, 0.5864, 0.5419, 0.545, 0.36, 0.4246, 0.4017, 0.3371, 0.2786, 0.3074, 0.4633, 0.4152, 0.1503, 0.3711, -0.0964, 0.432, 0.1244, -0.0729, 0.1849, 0.3971, 0.2741, 0.0914, 0.0689, 0.031, 0.3843, 0.1499, -0.5822, -0.2218, -0.1824, 0.1025, -0.0227, -0.751, -0.7247, 0.7934, 0.7934, 0.7932, 0.7931, 0.7928, 0.7928, 0.7928, 0.7926, 0.7924, 0.7923, 0.792, 0.7919, 0.791, 0.7909, 0.7907, 0.7906, 0.7903, 0.7902, 0.79, 0.789, 0.7888, 0.7888, 0.7887, 0.7886, 0.7886, 0.7884, 0.7879, 0.7879, 0.7878, 0.7878, 0.7514, 0.7345, 0.655, 0.7234, 0.6996, 0.739, 0.6831, 0.5985, 0.731, 0.4947, 0.4284, 0.4854, 0.6855, 0.457, 0.1056, 0.4901, 0.0817, 0.4081, 0.5496, 0.216, 0.3413, 0.184, -0.2189, -0.7448, -0.0389, -0.1749], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.2777, -3.8972, -4.8204, -5.3407, -5.4904, -5.5354, -5.5203, -5.5746, -5.6071, -5.7774, -5.7486, -5.7972, -5.9711, -5.9711, -6.0449, -6.045, -6.0075, -6.0974, -6.0975, -6.0842, -6.2267, -6.2267, -6.2116, -6.2268, -6.2578, -6.2578, -6.2422, -6.2579, -6.2738, -6.258, -6.258, -5.926, -5.0121, -5.5386, -3.2253, -4.5739, -4.7392, -4.771, -4.5669, -4.712, -5.5785, -5.365, -4.2426, -5.2604, -3.7348, -5.5107, -4.7288, -4.2847, -4.9925, -5.4907, -5.2326, -4.9299, -5.0065, -4.9611, -5.4684, -5.2104, -4.6168, -5.0836, -5.2542, -5.3934, -5.4092, -5.2935, -5.4168, -3.7538, -3.902, -4.2531, -4.319, -4.5682, -4.6557, -4.6634, -4.9106, -5.0307, -5.0308, -5.2745, -5.3328, -5.7006, -5.7337, -5.8153, -5.8033, -5.9043, -5.973, -6.0316, -6.2316, -6.1273, -6.2884, -6.2319, -6.1784, -6.3486, -6.3488, -6.4131, -6.4581, -6.4814, -6.4814, -3.0018, -4.3906, -3.4885, -5.1857, -4.9132, -5.6204, -4.9945, -4.5966, -5.7708, -4.0479, -3.6061, -4.2067, -5.4531, -4.4195, -3.5328, -5.0181, -4.1301, -4.9047, -5.2275, -4.6458, -5.0118, -4.8877, -4.6118, -4.3301, -5.031, -5.0281]}, \"token.table\": {\"Topic\": [2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [1.001045365785992, 0.9947323242310016, 0.9968492397696093, 0.05769450512078719, 0.9423435836395241, 0.9968079735705199, 0.9971666128320523, 0.9976493445632567, 0.3207315736891175, 0.6786004874896064, 0.999640782011168, 0.987126434663661, 0.9995601541847998, 0.26498075480067396, 0.7350493540703628, 1.0033831373811397, 0.6003128023325429, 0.3985269864224445, 0.9885794484531996, 1.0045940291023434, 1.0008344821212112, 0.9950968877906882, 0.9977458434628346, 0.9934810720223528, 0.995212995853855, 0.7676056744774951, 0.23110708478892325, 0.8678350890557898, 0.12597606131455014, 0.9948595616172302, 0.7939939502933431, 0.20430253691758535, 0.9949802311977017, 0.9988856891181938, 0.9979820193097516, 0.9991674388566041, 0.6204415032089504, 0.37951404663640265, 0.9987339383417925, 0.816001564956323, 0.1860003567179854, 0.9999227028492923, 0.994859721161566, 0.9871228619714911, 0.9975751925078673, 0.10022838617174297, 0.894345599686322, 0.7213298083967186, 0.27869560778964125, 0.6364901042069581, 0.36332416096380266, 0.30654680535326917, 0.6939617863518793, 0.9948569550568375, 0.9998886490084247, 0.21483976098890303, 0.7841651276094961, 0.9860836180712635, 0.011205495659900722, 0.8297146332253482, 0.17240823547539702, 0.9960164407564245, 0.9948584180953598, 0.9961057780832486, 0.8052629377766486, 0.19697079053529534, 0.9948576065613176, 0.2627169152161095, 0.7388913240453081, 1.0010461467553127, 0.5857951552466028, 0.4127193139237429, 0.998603691168632, 0.5097317460995182, 0.49064487454953626, 1.004592438066745, 0.608199734788067, 0.39330249516295, 0.36557473532274753, 0.6362406451290126, 0.05542991017222627, 0.9423084729278465, 0.8187472902758509, 0.18194384228352242, 0.9977095141416439, 0.04187608527771404, 0.9581816122866771, 0.9950975680810826, 0.9963562053837535, 0.9960152844658898, 0.6604800018278175, 0.3420342866608341, 0.4563102314191882, 0.5438581246565906, 0.9960905513662168, 0.9898952574398703, 0.9950978931379527, 0.9949133368771217, 0.17707348496125516, 0.8223412691421003, 1.0031075979558328, 0.49735354569749096, 0.5024156683000099, 0.9921793028642739, 0.7452754227827055, 0.2543000598008556, 0.6372562641074123, 0.36465219557257483, 0.9990536512919882, 1.0002021945236528, 0.9986274261754686, 0.9854961486452021, 0.998969983280198, 0.9989449221508004, 1.0031076107367762, 0.05876573951947508, 0.9430502008601477, 0.2578619138244547, 0.7407669524411608, 0.13003185556576513, 0.870374517093428, 0.9983048334405347, 0.534217598355304, 0.4651690652345504, 0.9413605563768342, 0.058378949232671895, 0.09114673146652405, 0.9114673146652406, 0.9968068596757013, 0.9980748148319898, 0.9966283504996758, 0.9963565402028931, 0.4387076407831274, 0.5612877168842954, 0.9978424891420786, 0.9965921209505475, 0.7854547845459903, 0.21481498309202787, 0.2859027669240614, 0.7136656853753288, 0.9411130915044624, 0.05586516068291329, 0.7236537484911812, 0.2750730622919695, 0.06741429975025633, 0.931543051094451, 0.8430123724946138, 0.15846097227342365, 1.0038965363230223, 0.10697398838022779, 0.8946915391800869, 0.8382935066646968, 0.16272756305844113, 0.5659104893022272, 0.433619725569239], \"Term\": [\"amazon\", \"around\", \"arrive\", \"available\", \"available\", \"away\", \"bar\", \"bath\", \"best\", \"best\", \"body\", \"bring\", \"brush\", \"buy\", \"buy\", \"christmas\", \"clean\", \"clean\", \"coat\", \"cologne\", \"conditioner\", \"curly\", \"dandruff\", \"de\", \"difference\", \"dry\", \"dry\", \"easy\", \"easy\", \"end\", \"even\", \"even\", \"expect\", \"eye\", \"face\", \"favorite\", \"feel\", \"feel\", \"fine\", \"first\", \"first\", \"fragrance\", \"free\", \"friend\", \"gel\", \"gift\", \"gift\", \"give\", \"give\", \"good\", \"good\", \"great\", \"great\", \"gum\", \"hair\", \"hard\", \"hard\", \"head\", \"head\", \"help\", \"help\", \"hold\", \"issue\", \"job\", \"keep\", \"keep\", \"kid\", \"last\", \"last\", \"lavender\", \"leave\", \"leave\", \"less\", \"like\", \"like\", \"lip\", \"little\", \"little\", \"long\", \"long\", \"longer\", \"longer\", \"look\", \"look\", \"lotion\", \"love\", \"love\", \"minute\", \"mouth\", \"mouthwash\", \"much\", \"much\", \"nice\", \"nice\", \"perfume\", \"polish\", \"powder\", \"pre\", \"price\", \"price\", \"pricey\", \"product\", \"product\", \"provence\", \"really\", \"really\", \"recommend\", \"recommend\", \"scalp\", \"scent\", \"sell\", \"seller\", \"shampoo\", \"shave\", \"shipping\", \"shower\", \"shower\", \"skin\", \"skin\", \"smell\", \"smell\", \"soap\", \"soft\", \"soft\", \"start\", \"start\", \"store\", \"store\", \"teeth\", \"thank\", \"thanks\", \"thin\", \"time\", \"time\", \"toothpaste\", \"top\", \"use\", \"use\", \"wash\", \"wash\", \"water\", \"water\", \"well\", \"well\", \"wish\", \"wish\", \"without\", \"without\", \"woman\", \"wonderful\", \"wonderful\", \"work\", \"work\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2410112171632704787172338\", ldavis_el2410112171632704787172338_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2410112171632704787172338\", ldavis_el2410112171632704787172338_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2410112171632704787172338\", ldavis_el2410112171632704787172338_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=               x           y  topics  cluster       Freq\n",
       "topic                                                   \n",
       "1     -60.312317  141.369629       1        1  54.789188\n",
       "0      60.312561 -141.369141       2        1  45.210812, topic_info=    Category         Freq   Term        Total  loglift  logprob\n",
       "498  Default  1408.000000   love  1408.000000  30.0000  30.0000\n",
       "385  Default  1242.000000   hair  1242.000000  29.0000  29.0000\n",
       "754  Default   636.000000  scent   636.000000  28.0000  28.0000\n",
       "101  Default   549.000000   body   549.000000  27.0000  27.0000\n",
       "809  Default   953.000000  smell   953.000000  26.0000  26.0000\n",
       "..       ...          ...    ...          ...      ...      ...\n",
       "572   Topic2   204.852866   nice   376.936540   0.1840  -4.8877\n",
       "370   Topic2   269.937582   good   743.138027  -0.2189  -4.6118\n",
       "932   Topic2   357.765221    use  1666.550419  -0.7448  -4.3301\n",
       "994   Topic2   177.499220   year   408.191762  -0.0389  -5.0310\n",
       "311   Topic2   178.017761   feel   469.020848  -0.1749  -5.0281\n",
       "\n",
       "[149 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "31        2  1.001045     amazon\n",
       "47        1  0.994732     around\n",
       "48        2  0.996849     arrive\n",
       "52        1  0.057695  available\n",
       "52        2  0.942344  available\n",
       "...     ...       ...        ...\n",
       "982       2  0.894692  wonderful\n",
       "985       1  0.838294       work\n",
       "985       2  0.162728       work\n",
       "994       1  0.565910       year\n",
       "994       2  0.433620       year\n",
       "\n",
       "[155 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df_beauty_reviews\n",
    "#define a Bag-of-Words vecgtorizer\n",
    "bow_vectorizer_news = CountVectorizer(max_features=1000)\n",
    "\n",
    "#vectorize data\n",
    "bow_news_corpus = bow_vectorizer_news.fit_transform(corpus)\n",
    "    \n",
    "lda_news = LatentDirichletAllocation(n_components=2, max_iter=100,\n",
    "                                     doc_topic_prior = 0.25,\n",
    "                                     topic_word_prior = 0.25).fit(bow_news_corpus)\n",
    "no_top_words_news = 10\n",
    "display_topics(lda_news, bow_vectorizer_news.get_feature_names(), no_top_words_news)\n",
    "#prepare to display result in the Jupyter notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#run the visualization [mds is a function to use for visualizing the \"distance\" between topics]\n",
    "pyLDAvis.sklearn.prepare(lda_news, bow_news_corpus, bow_vectorizer_news, mds='tsne')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
